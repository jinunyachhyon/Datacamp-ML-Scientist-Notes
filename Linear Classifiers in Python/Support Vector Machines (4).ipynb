{"cells":[{"source":"# Support Vector Machines","metadata":{},"cell_type":"markdown","id":"d16f21e2-a508-4ac4-b6b3-a80be935ca27"},{"source":"## What is an SVM?\n- Trained using the hinge loss and L2 regularization\n![image-10](image-10.png)\n\nIf the training example falls in this \"zero loss\" region, it doesn't contribute to the fit; even if they are removed, nothing would change. **This is a key property of SVM.**","metadata":{},"cell_type":"markdown","id":"6caf3e5f-296a-4dad-abbb-fa42c295e58d"},{"source":"- Support vector: a training example **not** in the flat part of the loss diagram.\n- Support vector: an example that is incorrectly classified **or** close to the boundary\n\n![image-11](image-11.png)\n\nIn the figure, support vectors are shown with yellow circles around them.","metadata":{},"cell_type":"markdown","id":"b39311b1-c311-4064-8d6a-80ab9a9b1e69"},{"source":"## Kernel SVMs \nFast processing -- because clever algorithms whose running time only scales with the number of support vectors, rather than the total number of training examples.","metadata":{},"cell_type":"markdown","id":"68f12306-036d-4112-a3c6-7a1b07e461dd"},{"source":"### SVM Kernel Functions\nSVM algorithms use a set of mathematical functions that are defined as the kernel. The function of kernel is to take data as input and transform it into the required form\n\n**default is kernel='rbf'**\n\nRBF = Radial Basis Function","metadata":{},"cell_type":"markdown","id":"79ad9966-1575-4b0c-b6a1-bf1cc0a71199"},{"source":"from sklearn.datasets import load_digits\ndigits = load_digits()\nX = digits.data\ny = digits.target","metadata":{"executionTime":504,"lastSuccessfullyExecutedCode":"from sklearn.datasets import load_digits\ndigits = load_digits()\nX = digits.data\ny = digits.target"},"cell_type":"code","id":"74e8124f-590c-4787-8659-a4b493af0af7","execution_count":1,"outputs":[]},{"source":"from sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n\n# Instantiate an RBF SVM\nsvm = SVC()\n\n# Instantiate the GridSearchCV object and run the search\n# gamma controls the smoothness of the boundary, by decreasing gamma, we can make the boundaries smoother\nparameters = {'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1]}\nsearcher = GridSearchCV(svm, parameters)\nsearcher.fit(X,y)\n\n# Report the best parameters\nprint(\"Best CV params\", searcher.best_params_)","metadata":{"executionTime":4267,"lastSuccessfullyExecutedCode":"from sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n\n# Instantiate an RBF SVM\nsvm = SVC()\n\n# Instantiate the GridSearchCV object and run the search\n# gamma controls the smoothness of the boundary, by decreasing gamma, we can make the boundaries smoother\nparameters = {'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1]}\nsearcher = GridSearchCV(svm, parameters)\nsearcher.fit(X,y)\n\n# Report the best parameters\nprint(\"Best CV params\", searcher.best_params_)"},"cell_type":"code","id":"49c63e38-b3e6-4825-b09c-89308a1b6222","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":"Best CV params {'gamma': 0.001}\n"}]},{"source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state=42)\n\n# Instantiate an RBF SVM\nsvm = SVC()\n\n# Instantiate the GridSearchCV object and run the search\n# C hyperparameter controls regularization\nparameters = {'C':[0.1, 1, 10], 'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1]}\nsearcher = GridSearchCV(svm, parameters)\nsearcher.fit(X_train,y_train)\n\n# Report the best parameters and the corresponding score\nprint(\"Best CV params\", searcher.best_params_)\nprint(\"Best CV accuracy\", searcher.best_score_)\n\n# Report the test accuracy using these best parameters\nprint(\"Test accuracy of best grid search hypers:\", searcher.score(X_test,y_test))","metadata":{"executionTime":6336,"lastSuccessfullyExecutedCode":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state=42)\n\n# Instantiate an RBF SVM\nsvm = SVC()\n\n# Instantiate the GridSearchCV object and run the search\n# C hyperparameter controls regularization\nparameters = {'C':[0.1, 1, 10], 'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1]}\nsearcher = GridSearchCV(svm, parameters)\nsearcher.fit(X_train,y_train)\n\n# Report the best parameters and the corresponding score\nprint(\"Best CV params\", searcher.best_params_)\nprint(\"Best CV accuracy\", searcher.best_score_)\n\n# Report the test accuracy using these best parameters\nprint(\"Test accuracy of best grid search hypers:\", searcher.score(X_test,y_test))"},"cell_type":"code","id":"c50b6850-1a86-4b81-9cca-aeb8abcdeaf3","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":"Best CV params {'C': 10, 'gamma': 0.001}\nBest CV accuracy 0.988861352058378\nTest accuracy of best grid search hypers: 0.9911111111111112\n"}]},{"source":"## Comparing logistic regression and SVM\n![image-12](image-12.png)\n","metadata":{},"cell_type":"markdown","id":"88e74ea0-2caa-45a0-9507-f5aac15732ff"},{"source":"### SGDClassifier \n`SGDClassifier`: scales well to large datasets\n- `SDGClassifier` hyperparameter `alpha` is like `1/C`\n\nTo switch between logistic regression and linear SVM, one only has to set the loss hyperparamter of the SGDClassifier.\n\n`logreg = SGDClassifier(loss='log_loss')` #logistic regression\n\n`linsvm = SGDClassifier(loss='hinge')` #linear SVM","metadata":{},"cell_type":"markdown","id":"09dcd758-dec0-498d-b6fc-7f62b5534666"},{"source":"from sklearn.linear_model import SGDClassifier\n\n# We set random_state=0 for reproducibility \nlinear_classifier = SGDClassifier(random_state=0)\n\n# Instantiate the GridSearchCV object and run the search\nparameters = {'alpha':[0.00001, 0.0001, 0.001, 0.01, 0.1, 1], \n             'loss':['hinge','log_loss']}\nsearcher = GridSearchCV(linear_classifier, parameters, cv=10)\nsearcher.fit(X_train, y_train)\n\n# Report the best parameters and the corresponding score\nprint(\"Best CV params\", searcher.best_params_)\nprint(\"Best CV accuracy\", searcher.best_score_)\nprint(\"Test accuracy of best grid search hypers:\", searcher.score(X_test, y_test))","metadata":{"executionTime":6429,"lastSuccessfullyExecutedCode":"from sklearn.linear_model import SGDClassifier\n\n# We set random_state=0 for reproducibility \nlinear_classifier = SGDClassifier(random_state=0)\n\n# Instantiate the GridSearchCV object and run the search\nparameters = {'alpha':[0.00001, 0.0001, 0.001, 0.01, 0.1, 1], \n             'loss':['hinge','log_loss']}\nsearcher = GridSearchCV(linear_classifier, parameters, cv=10)\nsearcher.fit(X_train, y_train)\n\n# Report the best parameters and the corresponding score\nprint(\"Best CV params\", searcher.best_params_)\nprint(\"Best CV accuracy\", searcher.best_score_)\nprint(\"Test accuracy of best grid search hypers:\", searcher.score(X_test, y_test))"},"cell_type":"code","id":"026805b2-f39c-47f3-a9cb-6058e3a64684","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":"Best CV params {'alpha': 0.1, 'loss': 'log_loss'}\nBest CV accuracy 0.9517081260364844\nTest accuracy of best grid search hypers: 0.9644444444444444\n"}]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}