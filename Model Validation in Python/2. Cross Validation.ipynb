{"cells":[{"source":"# Cross Validation","metadata":{},"cell_type":"markdown","id":"0ebb643e-d10b-4d91-a61b-03dc6a611f6f"},{"source":"import pandas as pd\ncandy = pd.read_csv('datasets/candy-data.csv')\ncandy.head()","metadata":{"executionTime":1243,"lastSuccessfullyExecutedCode":"import pandas as pd\ncandy = pd.read_csv('datasets/candy-data.csv')\ncandy.head()"},"cell_type":"code","id":"2d8c244b-47e4-4be2-bf18-b0a1cacd6d8a","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"application/com.datacamp.data-table.v1+json":{"table":{"schema":{"fields":[{"name":"index","type":"integer"},{"name":"competitorname","type":"string"},{"name":"chocolate","type":"integer"},{"name":"fruity","type":"integer"},{"name":"caramel","type":"integer"},{"name":"peanutyalmondy","type":"integer"},{"name":"nougat","type":"integer"},{"name":"crispedricewafer","type":"integer"},{"name":"hard","type":"integer"},{"name":"bar","type":"integer"},{"name":"pluribus","type":"integer"},{"name":"sugarpercent","type":"number"},{"name":"pricepercent","type":"number"},{"name":"winpercent","type":"number"}],"primaryKey":["index"],"pandas_version":"1.4.0"},"data":[{"index":0,"competitorname":"100 Grand","chocolate":1,"fruity":0,"caramel":1,"peanutyalmondy":0,"nougat":0,"crispedricewafer":1,"hard":0,"bar":1,"pluribus":0,"sugarpercent":0.73199999,"pricepercent":0.86000001,"winpercent":66.971725},{"index":1,"competitorname":"3 Musketeers","chocolate":1,"fruity":0,"caramel":0,"peanutyalmondy":0,"nougat":1,"crispedricewafer":0,"hard":0,"bar":1,"pluribus":0,"sugarpercent":0.60399997,"pricepercent":0.51099998,"winpercent":67.602936},{"index":2,"competitorname":"One dime","chocolate":0,"fruity":0,"caramel":0,"peanutyalmondy":0,"nougat":0,"crispedricewafer":0,"hard":0,"bar":0,"pluribus":0,"sugarpercent":0.011,"pricepercent":0.116,"winpercent":32.261086},{"index":3,"competitorname":"One quarter","chocolate":0,"fruity":0,"caramel":0,"peanutyalmondy":0,"nougat":0,"crispedricewafer":0,"hard":0,"bar":0,"pluribus":0,"sugarpercent":0.011,"pricepercent":0.51099998,"winpercent":46.116505},{"index":4,"competitorname":"Air Heads","chocolate":0,"fruity":1,"caramel":0,"peanutyalmondy":0,"nougat":0,"crispedricewafer":0,"hard":0,"bar":0,"pluribus":0,"sugarpercent":0.90600002,"pricepercent":0.51099998,"winpercent":52.341465}]},"total_rows":5,"truncation_type":null},"text/plain":"  competitorname  chocolate  fruity  ...  sugarpercent  pricepercent  winpercent\n0      100 Grand          1       0  ...         0.732         0.860   66.971725\n1   3 Musketeers          1       0  ...         0.604         0.511   67.602936\n2       One dime          0       0  ...         0.011         0.116   32.261086\n3    One quarter          0       0  ...         0.011         0.511   46.116505\n4      Air Heads          0       1  ...         0.906         0.511   52.341465\n\n[5 rows x 13 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>competitorname</th>\n      <th>chocolate</th>\n      <th>fruity</th>\n      <th>caramel</th>\n      <th>peanutyalmondy</th>\n      <th>nougat</th>\n      <th>crispedricewafer</th>\n      <th>hard</th>\n      <th>bar</th>\n      <th>pluribus</th>\n      <th>sugarpercent</th>\n      <th>pricepercent</th>\n      <th>winpercent</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100 Grand</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.732</td>\n      <td>0.860</td>\n      <td>66.971725</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3 Musketeers</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.604</td>\n      <td>0.511</td>\n      <td>67.602936</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>One dime</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.011</td>\n      <td>0.116</td>\n      <td>32.261086</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>One quarter</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.011</td>\n      <td>0.511</td>\n      <td>46.116505</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Air Heads</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.906</td>\n      <td>0.511</td>\n      <td>52.341465</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"source":"X = candy.drop(['competitorname','winpercent'],axis=1)\ny = candy['winpercent']","metadata":{"executionTime":0,"lastSuccessfullyExecutedCode":"X = candy.drop(['competitorname','winpercent'],axis=1)\ny = candy['winpercent']"},"cell_type":"code","id":"d7ca5545-70f4-4bed-a1d2-34b3009282a3","execution_count":3,"outputs":[]},{"source":"from sklearn.model_selection import KFold\n\n# Use KFold\nkf = KFold(n_splits=5, shuffle=True, random_state=1111)\n\n# Create splits -- Generate indices to split data into training and test set. \nsplits = kf.split(X)\n\n# Print the number of indices\nfor train_index, val_index in splits:\n    print(\"Number of training indices: %s\" % len(train_index))\n    print(\"Number of validation indices: %s\" % len(val_index))","metadata":{"executionTime":398,"lastSuccessfullyExecutedCode":"from sklearn.model_selection import KFold\n\n# Use KFold\nkf = KFold(n_splits=5, shuffle=True, random_state=1111)\n\n# Create splits -- Generate indices to split data into training and test set. \nsplits = kf.split(X)\n\n# Print the number of indices\nfor train_index, val_index in splits:\n    print(\"Number of training indices: %s\" % len(train_index))\n    print(\"Number of validation indices: %s\" % len(val_index))"},"cell_type":"code","id":"00842505-808b-4f64-bc82-31c19fdd1fea","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":"Number of training indices: 68\nNumber of validation indices: 17\nNumber of training indices: 68\nNumber of validation indices: 17\nNumber of training indices: 68\nNumber of validation indices: 17\nNumber of training indices: 68\nNumber of validation indices: 17\nNumber of training indices: 68\nNumber of validation indices: 17\n"}]},{"source":"from sklearn.model_selection import KFold\n\n# Use KFold\nkf = KFold(n_splits=5, shuffle=True, random_state=1111)\n\n# Create splits -- Generate indices to split data into training and test set. \nsplits = kf.split(X)\n\n# Print the indices in each training and validation sets at each iteration\nfor train_index, val_index in splits:\n    print(\"Number of training indices: %s\" % (train_index))\n    print(\"Number of validation indices: %s\" % (val_index))","metadata":{"executionTime":32,"lastSuccessfullyExecutedCode":"from sklearn.model_selection import KFold\n\n# Use KFold\nkf = KFold(n_splits=5, shuffle=True, random_state=1111)\n\n# Create splits -- Generate indices to split data into training and test set. \nsplits = kf.split(X)\n\n# Print the indices in each training and validation sets at each iteration\nfor train_index, val_index in splits:\n    print(\"Number of training indices: %s\" % (train_index))\n    print(\"Number of validation indices: %s\" % (val_index))"},"cell_type":"code","id":"18e135de-3ec2-4887-a762-c68d5313f4c3","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":"Number of training indices: [ 0  2  3  4  5  7  8  9 10 11 12 13 14 17 18 20 21 22 23 24 25 27 28 29\n 31 32 33 34 35 36 37 39 40 42 44 45 46 47 48 49 50 51 52 53 54 55 56 57\n 60 61 62 63 64 67 68 70 71 73 74 75 76 77 78 79 80 81 82 83]\nNumber of validation indices: [ 1  6 15 16 19 26 30 38 41 43 58 59 65 66 69 72 84]\nNumber of training indices: [ 0  1  2  4  5  6  8  9 11 12 14 15 16 17 18 19 20 21 22 23 24 25 26 28\n 29 30 31 32 33 34 36 38 40 41 42 43 44 45 48 50 51 52 53 54 55 56 58 59\n 60 61 62 63 64 65 66 67 68 69 70 71 72 73 75 78 80 81 82 84]\nNumber of validation indices: [ 3  7 10 13 27 35 37 39 46 47 49 57 74 76 77 79 83]\nNumber of training indices: [ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 26 27 28 30 32 34 35 36 37 38 39 40 41 42 43 45 46 47 49 50 51 52 54 55\n 57 58 59 61 62 64 65 66 69 71 72 73 74 76 77 79 81 82 83 84]\nNumber of validation indices: [ 2 25 29 31 33 44 48 53 56 60 63 67 68 70 75 78 80]\nNumber of training indices: [ 1  2  3  6  7  8 10 11 12 13 14 15 16 19 20 22 24 25 26 27 28 29 30 31\n 33 34 35 37 38 39 41 43 44 46 47 48 49 51 52 53 55 56 57 58 59 60 61 62\n 63 65 66 67 68 69 70 72 73 74 75 76 77 78 79 80 81 82 83 84]\nNumber of validation indices: [ 0  4  5  9 17 18 21 23 32 36 40 42 45 50 54 64 71]\nNumber of training indices: [ 0  1  2  3  4  5  6  7  9 10 13 15 16 17 18 19 21 23 25 26 27 29 30 31\n 32 33 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 53 54 56 57 58 59\n 60 63 64 65 66 67 68 69 70 71 72 74 75 76 77 78 79 80 83 84]\nNumber of validation indices: [ 8 11 12 14 20 22 24 28 34 51 52 55 61 62 73 81 82]\n"}]},{"source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\nrfc = RandomForestRegressor(n_estimators=25, random_state=1111)\n\n# Create splits -- Generate indices to split data into training and test set. \nsplits = kf.split(X)\n\n# Access the training and validation indices of splits\nfor train_index, val_index in splits:\n    # Setup the training and validation data\n    X_train, y_train = X.iloc[train_index], y.iloc[train_index]\n    X_val, y_val = X.iloc[val_index], y.iloc[val_index]\n    # Fit the random forest model\n    rfc.fit(X_train, y_train)\n    # Make predictions, and print the accuracy\n    predictions = rfc.predict(X_val)\n    print(\"Split MSE: \" + str(mean_squared_error(y_val, predictions)))","metadata":{"executionTime":54,"lastSuccessfullyExecutedCode":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\nrfc = RandomForestRegressor(n_estimators=25, random_state=1111)\n\n# Create splits -- Generate indices to split data into training and test set. \nsplits = kf.split(X)\n\n# Access the training and validation indices of splits\nfor train_index, val_index in splits:\n    # Setup the training and validation data\n    X_train, y_train = X.iloc[train_index], y.iloc[train_index]\n    X_val, y_val = X.iloc[val_index], y.iloc[val_index]\n    # Fit the random forest model\n    rfc.fit(X_train, y_train)\n    # Make predictions, and print the accuracy\n    predictions = rfc.predict(X_val)\n    print(\"Split MSE: \" + str(mean_squared_error(y_val, predictions)))"},"cell_type":"code","id":"5c64438e-6345-414f-a5f5-8c8951683533","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":"Split MSE: 150.99298148707666\nSplit MSE: 171.22206240542593\nSplit MSE: 131.72569156195593\nSplit MSE: 80.61940183841385\nSplit MSE: 221.63020627476214\n"}]},{"source":"# Sklearn's cross_val_score()","metadata":{},"cell_type":"markdown","id":"a20b43eb-0c83-41d3-8c01-b2190e4943e1"},{"source":"# Instruction 1: Load the cross-validation method\nfrom sklearn.model_selection import cross_val_score\n\n# Instruction 2: Load the random forest regression model\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Instruction 3: Load the mean squared error method\n# Instruction 4: Load the function for creating a scorer\nfrom sklearn.metrics import mean_squared_error, make_scorer","metadata":{"executionTime":149,"lastSuccessfullyExecutedCode":"# Instruction 1: Load the cross-validation method\nfrom sklearn.model_selection import cross_val_score\n\n# Instruction 2: Load the random forest regression model\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Instruction 3: Load the mean squared error method\n# Instruction 4: Load the function for creating a scorer\nfrom sklearn.metrics import mean_squared_error, make_scorer"},"cell_type":"code","id":"9a2137d4-dfa0-420f-8cf3-86f32111ce36","execution_count":7,"outputs":[]},{"source":"rfc = RandomForestRegressor(n_estimators=25, random_state=1111)\nmse = make_scorer(mean_squared_error)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Set up cross_val_score\ncv = cross_val_score(estimator=rfc,\n                     X=X_train,\n                     y=y_train,\n                     cv=10,\n                     scoring=mse)\n\n# Print the mean error\nprint(cv.mean())","metadata":{"executionTime":34,"lastSuccessfullyExecutedCode":"rfc = RandomForestRegressor(n_estimators=25, random_state=1111)\nmse = make_scorer(mean_squared_error)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Set up cross_val_score\ncv = cross_val_score(estimator=rfc,\n                     X=X_train,\n                     y=y_train,\n                     cv=10,\n                     scoring=mse)\n\n# Print the mean error\nprint(cv.mean())"},"cell_type":"code","id":"214946d4-206b-4bf6-8160-0ab26f2bb8ee","execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":"125.08498113726314\n"}]},{"source":"cv","metadata":{"executionTime":471,"lastSuccessfullyExecutedCode":"cv"},"cell_type":"code","id":"d6e90f5e-e1cf-4556-bc08-8d790a26a176","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"array([114.95580647, 156.06194645, 148.50119665,  95.84285862,\n       126.72936664,  59.27083503, 109.66617657, 102.84197154,\n       137.89116868, 199.08848472])"},"metadata":{}}]},{"source":"# Leave-one-out-cross-validation (LOOCV)\n\n![image-7](image-7.png)\n\n- The name says it all. In leave-one-out-cross-validation, we are going to implement KFold cross-validation, where k is equal to n, the number of observations in the data. This means that every single point will be used in a validation set, completely by itself. For the first model, we will use all of the data for training, except for the first point, which will be used for validation. In model 2, we leave only the second data point out, in model three, the third data point, and so on. We create n models, for n-observations in the data.","metadata":{},"cell_type":"markdown","id":"cb391fd3-46e0-44b6-8861-d10360f62546"},{"source":"import numpy as np\nfrom sklearn.metrics import mean_absolute_error, make_scorer\n\n# Create scorer\nmae_scorer = make_scorer(mean_absolute_error)\n\nrfr = RandomForestRegressor(n_estimators=15, random_state=1111)\n\n# Implement LOOCV\nscores = cross_val_score(rfr, X=X, y=y, cv=X.shape[0], scoring=mae_scorer)\n\n# Print the mean and standard deviation\nprint(\"The mean of the errors is: %s.\" % np.mean(scores))\nprint(\"The standard deviation of the errors is: %s.\" % np.std(scores))","metadata":{"executionTime":58,"lastSuccessfullyExecutedCode":"import numpy as np\nfrom sklearn.metrics import mean_absolute_error, make_scorer\n\n# Create scorer\nmae_scorer = make_scorer(mean_absolute_error)\n\nrfr = RandomForestRegressor(n_estimators=15, random_state=1111)\n\n# Implement LOOCV\nscores = cross_val_score(rfr, X=X, y=y, cv=X.shape[0], scoring=mae_scorer)\n\n# Print the mean and standard deviation\nprint(\"The mean of the errors is: %s.\" % np.mean(scores))\nprint(\"The standard deviation of the errors is: %s.\" % np.std(scores))"},"cell_type":"code","id":"4863be1f-a3f8-4035-803d-0b617dece9c4","execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":"The mean of the errors is: 9.52044832324183.\nThe standard deviation of the errors is: 7.349020637882744.\n"}]}],"metadata":{"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}