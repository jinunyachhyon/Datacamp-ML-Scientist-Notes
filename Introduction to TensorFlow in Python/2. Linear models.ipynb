{"cells":[{"source":"# Input data","metadata":{},"cell_type":"markdown","id":"1624c32b-7146-46b4-a688-71cccae99bf9"},{"source":"# Import numpy and pandas\nimport numpy as np\nimport pandas as pd\n\n# Load data from csv\nhousing_csv = pd.read_csv('datasets/kc_house_data.csv')\n\n# Convert to numpy array\nhousing_array = np.array(housing_csv)","metadata":{"executionTime":84,"lastSuccessfullyExecutedCode":"# Import numpy and pandas\nimport numpy as np\nimport pandas as pd\n\n# Load data from csv\nhousing_csv = pd.read_csv('datasets/kc_house_data.csv')\n\n# Convert to numpy array\nhousing_array = np.array(housing_csv)"},"cell_type":"code","id":"27bcb4f0-fd94-4d85-a4e9-e7d910648748","execution_count":4,"outputs":[]},{"source":"housing_csv.head()","metadata":{"executionTime":244,"lastSuccessfullyExecutedCode":"housing_csv.head()"},"cell_type":"code","id":"66dc7841-9a9c-467c-b90b-b9f1d7743725","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"application/com.datacamp.data-table.v1+json":{"table":{"schema":{"fields":[{"name":"index","type":"integer"},{"name":"id","type":"integer"},{"name":"date","type":"string"},{"name":"price","type":"number"},{"name":"bedrooms","type":"integer"},{"name":"bathrooms","type":"number"},{"name":"sqft_living","type":"integer"},{"name":"sqft_lot","type":"integer"},{"name":"floors","type":"number"},{"name":"waterfront","type":"integer"},{"name":"view","type":"integer"},{"name":"condition","type":"integer"},{"name":"grade","type":"integer"},{"name":"sqft_above","type":"integer"},{"name":"sqft_basement","type":"integer"},{"name":"yr_built","type":"integer"},{"name":"yr_renovated","type":"integer"},{"name":"zipcode","type":"integer"},{"name":"lat","type":"number"},{"name":"long","type":"number"},{"name":"sqft_living15","type":"integer"},{"name":"sqft_lot15","type":"integer"}],"primaryKey":["index"],"pandas_version":"1.4.0"},"data":[{"index":0,"id":7129300520,"date":"20141013T000000","price":221900,"bedrooms":3,"bathrooms":1,"sqft_living":1180,"sqft_lot":5650,"floors":1,"waterfront":0,"view":0,"condition":3,"grade":7,"sqft_above":1180,"sqft_basement":0,"yr_built":1955,"yr_renovated":0,"zipcode":98178,"lat":47.5112,"long":-122.257,"sqft_living15":1340,"sqft_lot15":5650},{"index":1,"id":6414100192,"date":"20141209T000000","price":538000,"bedrooms":3,"bathrooms":2.25,"sqft_living":2570,"sqft_lot":7242,"floors":2,"waterfront":0,"view":0,"condition":3,"grade":7,"sqft_above":2170,"sqft_basement":400,"yr_built":1951,"yr_renovated":1991,"zipcode":98125,"lat":47.721,"long":-122.319,"sqft_living15":1690,"sqft_lot15":7639},{"index":2,"id":5631500400,"date":"20150225T000000","price":180000,"bedrooms":2,"bathrooms":1,"sqft_living":770,"sqft_lot":10000,"floors":1,"waterfront":0,"view":0,"condition":3,"grade":6,"sqft_above":770,"sqft_basement":0,"yr_built":1933,"yr_renovated":0,"zipcode":98028,"lat":47.7379,"long":-122.233,"sqft_living15":2720,"sqft_lot15":8062},{"index":3,"id":2487200875,"date":"20141209T000000","price":604000,"bedrooms":4,"bathrooms":3,"sqft_living":1960,"sqft_lot":5000,"floors":1,"waterfront":0,"view":0,"condition":5,"grade":7,"sqft_above":1050,"sqft_basement":910,"yr_built":1965,"yr_renovated":0,"zipcode":98136,"lat":47.5208,"long":-122.393,"sqft_living15":1360,"sqft_lot15":5000},{"index":4,"id":1954400510,"date":"20150218T000000","price":510000,"bedrooms":3,"bathrooms":2,"sqft_living":1680,"sqft_lot":8080,"floors":1,"waterfront":0,"view":0,"condition":3,"grade":8,"sqft_above":1680,"sqft_basement":0,"yr_built":1987,"yr_renovated":0,"zipcode":98074,"lat":47.6168,"long":-122.045,"sqft_living15":1800,"sqft_lot15":7503}]},"total_rows":5,"truncation_type":null},"text/plain":"           id             date     price  ...     long  sqft_living15  sqft_lot15\n0  7129300520  20141013T000000  221900.0  ... -122.257           1340        5650\n1  6414100192  20141209T000000  538000.0  ... -122.319           1690        7639\n2  5631500400  20150225T000000  180000.0  ... -122.233           2720        8062\n3  2487200875  20141209T000000  604000.0  ... -122.393           1360        5000\n4  1954400510  20150218T000000  510000.0  ... -122.045           1800        7503\n\n[5 rows x 21 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>date</th>\n      <th>price</th>\n      <th>bedrooms</th>\n      <th>bathrooms</th>\n      <th>sqft_living</th>\n      <th>sqft_lot</th>\n      <th>floors</th>\n      <th>waterfront</th>\n      <th>view</th>\n      <th>condition</th>\n      <th>grade</th>\n      <th>sqft_above</th>\n      <th>sqft_basement</th>\n      <th>yr_built</th>\n      <th>yr_renovated</th>\n      <th>zipcode</th>\n      <th>lat</th>\n      <th>long</th>\n      <th>sqft_living15</th>\n      <th>sqft_lot15</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7129300520</td>\n      <td>20141013T000000</td>\n      <td>221900.0</td>\n      <td>3</td>\n      <td>1.00</td>\n      <td>1180</td>\n      <td>5650</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>7</td>\n      <td>1180</td>\n      <td>0</td>\n      <td>1955</td>\n      <td>0</td>\n      <td>98178</td>\n      <td>47.5112</td>\n      <td>-122.257</td>\n      <td>1340</td>\n      <td>5650</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6414100192</td>\n      <td>20141209T000000</td>\n      <td>538000.0</td>\n      <td>3</td>\n      <td>2.25</td>\n      <td>2570</td>\n      <td>7242</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>7</td>\n      <td>2170</td>\n      <td>400</td>\n      <td>1951</td>\n      <td>1991</td>\n      <td>98125</td>\n      <td>47.7210</td>\n      <td>-122.319</td>\n      <td>1690</td>\n      <td>7639</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5631500400</td>\n      <td>20150225T000000</td>\n      <td>180000.0</td>\n      <td>2</td>\n      <td>1.00</td>\n      <td>770</td>\n      <td>10000</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>6</td>\n      <td>770</td>\n      <td>0</td>\n      <td>1933</td>\n      <td>0</td>\n      <td>98028</td>\n      <td>47.7379</td>\n      <td>-122.233</td>\n      <td>2720</td>\n      <td>8062</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2487200875</td>\n      <td>20141209T000000</td>\n      <td>604000.0</td>\n      <td>4</td>\n      <td>3.00</td>\n      <td>1960</td>\n      <td>5000</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>7</td>\n      <td>1050</td>\n      <td>910</td>\n      <td>1965</td>\n      <td>0</td>\n      <td>98136</td>\n      <td>47.5208</td>\n      <td>-122.393</td>\n      <td>1360</td>\n      <td>5000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1954400510</td>\n      <td>20150218T000000</td>\n      <td>510000.0</td>\n      <td>3</td>\n      <td>2.00</td>\n      <td>1680</td>\n      <td>8080</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>8</td>\n      <td>1680</td>\n      <td>0</td>\n      <td>1987</td>\n      <td>0</td>\n      <td>98074</td>\n      <td>47.6168</td>\n      <td>-122.045</td>\n      <td>1800</td>\n      <td>7503</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"source":"housing_array","metadata":{"executionTime":93,"lastSuccessfullyExecutedCode":"housing_array"},"cell_type":"code","id":"a069094c-ebea-4ac0-ad98-fe1fa8b43089","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"array([[7129300520, '20141013T000000', 221900.0, ..., -122.257, 1340,\n        5650],\n       [6414100192, '20141209T000000', 538000.0, ..., -122.319, 1690,\n        7639],\n       [5631500400, '20150225T000000', 180000.0, ..., -122.233, 2720,\n        8062],\n       ...,\n       [1523300141, '20140623T000000', 402101.0, ..., -122.299, 1020,\n        2007],\n       [291310100, '20150116T000000', 400000.0, ..., -122.069, 1410,\n        1287],\n       [1523300157, '20141015T000000', 325000.0, ..., -122.299, 1020,\n        1357]], dtype=object)"},"metadata":{}}]},{"source":"# Import numpy and tensorflow with their standard aliases\nimport numpy as np\nimport tensorflow as tf\n\n# Use a numpy array to define price as a 32-bit float\nprice = np.array(housing_csv['price'], np.float32)\n\n# Define waterfront as a Boolean using cast\nwaterfront = tf.cast(housing_csv['waterfront'], tf.bool)\n\n# Print price and waterfront\nprint(price)\nprint(waterfront)","metadata":{"executionTime":53,"lastSuccessfullyExecutedCode":"# Import numpy and tensorflow with their standard aliases\nimport numpy as np\nimport tensorflow as tf\n\n# Use a numpy array to define price as a 32-bit float\nprice = np.array(housing_csv['price'], np.float32)\n\n# Define waterfront as a Boolean using cast\nwaterfront = tf.cast(housing_csv['waterfront'], tf.bool)\n\n# Print price and waterfront\nprint(price)\nprint(waterfront)"},"cell_type":"code","id":"a8a8dbf9-b784-4c7d-a5eb-28237e167833","execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":"[221900. 538000. 180000. ... 402101. 400000. 325000.]\ntf.Tensor([False False False ... False False False], shape=(21613,), dtype=bool)\n"},{"output_type":"stream","name":"stderr","text":"2023-05-20 11:20:25.815471: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n2023-05-20 11:20:25.815498: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n2023-05-20 11:20:25.815518: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (5855a770-b980-4a93-9254-7ae0a6563cbc): /proc/driver/nvidia/version does not exist\n2023-05-20 11:20:25.815839: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"}]},{"source":"**Note:** Notice that printing price yielded a numpy array; whereas printing waterfront yielded a tf.Tensor().","metadata":{},"cell_type":"markdown","id":"2f6b03cd-156f-4eb5-9b1f-3a8059b1bb23"},{"source":"# Loss functions\n- TensorFlow has operations for common loss functions\n    - Mean squared error(MSE)\n    - Mean absolute error(MAE)\n    - Huber error\n\n![image-3](image-3.png)\n","metadata":{},"cell_type":"markdown","id":"97664b1f-3e06-45b7-bfd1-802c15fbe51c"},{"source":"import tensorflow as tf\nfrom tensorflow import Variable\nfrom tensorflow import keras\n\n# Initialize a variable named scalar\nscalar = Variable(1.0, tf.float32)\n\n# Instantiate features\nfeatures = Variable([1.0,2.0,3.0,4.0,5.0], tf.float32)\n\n# Instantiate targets\ntargets = Variable([2.0,4.0,6.0,8.0,10.0], tf.float32)\n\n# Define the model\ndef model(scalar, features = features):\n  \treturn scalar * features\n\n# Define a loss function\ndef loss_function(scalar, features = features, targets = targets):\n\t# Compute the predicted values\n\tpredictions = model(scalar, features)\n    \n\t# Return the mean absolute error loss\n\treturn keras.losses.mae(targets, predictions)\n\n# Evaluate the loss function and print the loss\nprint(loss_function(scalar).numpy())","metadata":{"executionTime":87,"lastSuccessfullyExecutedCode":"import tensorflow as tf\nfrom tensorflow import Variable\nfrom tensorflow import keras\n\n# Initialize a variable named scalar\nscalar = Variable(1.0, tf.float32)\n\n# Instantiate features\nfeatures = Variable([1.0,2.0,3.0,4.0,5.0], tf.float32)\n\n# Instantiate targets\ntargets = Variable([2.0,4.0,6.0,8.0,10.0], tf.float32)\n\n# Define the model\ndef model(scalar, features = features):\n  \treturn scalar * features\n\n# Define a loss function\ndef loss_function(scalar, features = features, targets = targets):\n\t# Compute the predicted values\n\tpredictions = model(scalar, features)\n    \n\t# Return the mean absolute error loss\n\treturn keras.losses.mae(targets, predictions)\n\n# Evaluate the loss function and print the loss\nprint(loss_function(scalar).numpy())"},"cell_type":"code","id":"9f5e34e9-fd4a-4ea1-ac4b-95bc37870135","execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":"3.0\n"}]},{"source":"# Linear regression in TensorFlow","metadata":{},"cell_type":"markdown","id":"b7bd7fe3-f95b-4969-9637-764b73f6f850"},{"source":"# Define the targets and features\nprice = np.array(np.log(housing_csv['price']), np.float32)\nsize = np.array(np.log(housing_csv['sqft_living']),np.float32)\n\n# Define the intercept and slope\nintercept = tf.Variable(0.1, np.float32)\nslope = tf.Variable(0.1, np.float32)","metadata":{"executionTime":109,"lastSuccessfullyExecutedCode":"# Define the targets and features\nprice = np.array(np.log(housing_csv['price']), np.float32)\nsize = np.array(np.log(housing_csv['sqft_living']),np.float32)\n\n# Define the intercept and slope\nintercept = tf.Variable(0.1, np.float32)\nslope = tf.Variable(0.1, np.float32)"},"cell_type":"code","id":"30c94082-68d8-4deb-97e2-deee09b64f16","execution_count":22,"outputs":[]},{"source":"# Define a linear regression model\ndef linear_regression(intercept, slope, features =size):\n    return intercept + features*slope","metadata":{"executionTime":124,"lastSuccessfullyExecutedCode":"# Define a linear regression model\ndef linear_regression(intercept, slope, features =size):\n    return intercept + features*slope"},"cell_type":"code","id":"33d90d43-935a-442b-af7a-ab2ca7da1e30","execution_count":23,"outputs":[]},{"source":"# Compute the predicted values and loss\ndef loss_function(intercept, slope, targets=price, features=size):\n    predictions = linear_regression(intercept, slope)\n    return tf.keras.losses.mse(targets, predictions)","metadata":{"executionTime":115,"lastSuccessfullyExecutedCode":"# Compute the predicted values and loss\ndef loss_function(intercept, slope, targets=price, features=size):\n    predictions = linear_regression(intercept, slope)\n    return tf.keras.losses.mse(targets, predictions)"},"cell_type":"code","id":"34700b16-582f-4648-b5ef-63862a36d285","execution_count":24,"outputs":[]},{"source":"# Compute the loss for different slope and intercept values\nprint(loss_function(0.1, 0.1).numpy())\nprint(loss_function(0.1, 0.5).numpy())","metadata":{"executionTime":187,"lastSuccessfullyExecutedCode":"# Compute the loss for different slope and intercept values\nprint(loss_function(0.1, 0.1).numpy())\nprint(loss_function(0.1, 0.5).numpy())"},"cell_type":"code","id":"99370ea6-043b-4e6e-a875-737288b7ccb9","execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":"148.91295\n84.309006\n"}]},{"source":"# Define an optimization operation - Adam with learning rate of 0.5\nopt = tf.keras.optimizers.Adam(0.5)","metadata":{"executionTime":101,"lastSuccessfullyExecutedCode":"# Define an optimization operation - Adam with learning rate of 0.5\nopt = tf.keras.optimizers.Adam(0.5)"},"cell_type":"code","id":"fddd0a07-3ef2-48c4-8a20-206d36084112","execution_count":31,"outputs":[]},{"source":"intercept = tf.Variable(0.1)\nslope = tf.Variable(0.5)\n\n# Minimize the loss function and print the loss\nfor j in range(100): # Epoch=100\n    opt.minimize(lambda: loss_function(intercept,slope),\n                var_list=[intercept,slope])\n    if j%10==0: # Print loss_function for every 10th epoch\n        print(loss_function(intercept, slope))","metadata":{"executionTime":523,"lastSuccessfullyExecutedCode":"intercept = tf.Variable(0.1)\nslope = tf.Variable(0.5)\n\n# Minimize the loss function and print the loss\nfor j in range(100): # Epoch=100\n    opt.minimize(lambda: loss_function(intercept,slope),\n                var_list=[intercept,slope])\n    if j%10==0: # Print loss_function for every 10th epoch\n        print(loss_function(intercept, slope))"},"cell_type":"code","id":"0bff3f24-f11a-4b2f-9a4e-e6d4d8a1d7ef","execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":"tf.Tensor(11.748244, shape=(), dtype=float32)\ntf.Tensor(0.3285229, shape=(), dtype=float32)\ntf.Tensor(1.2011963, shape=(), dtype=float32)\ntf.Tensor(1.9263566, shape=(), dtype=float32)\ntf.Tensor(1.2263952, shape=(), dtype=float32)\ntf.Tensor(0.5787599, shape=(), dtype=float32)\ntf.Tensor(0.3191719, shape=(), dtype=float32)\ntf.Tensor(0.2504808, shape=(), dtype=float32)\ntf.Tensor(0.23634388, shape=(), dtype=float32)\ntf.Tensor(0.23297228, shape=(), dtype=float32)\n"}]},{"source":"**Code Break-down**\n\n- `opt.minimize`: This is the function call to an optimization method, which aims to minimize a given function.\n\n- `lambda: loss_function(intercept, slope)`: This lambda function defines the loss function to be minimized. The lambda function takes no arguments (lambda:) and calls the loss_function with the intercept and slope parameters.\n\n- `var_list=[intercept, slope]`: This parameter specifies the variables (intercept and slope) that will be optimized during the minimization process. The optimization algorithm will search for the optimal values of these variables that minimize the given loss function.","metadata":{},"cell_type":"markdown","id":"2d1bf05d-bb1e-4c7f-984b-70f5a04c56c1"},{"source":"# Multiple linear regression with TensorFlow","metadata":{},"cell_type":"markdown","id":"c14e2d81-28f2-4adf-83ef-6faab9ce9db7"},{"source":"# Initialize target and features\ntarget = np.array(np.log(housing_csv['price'])) # Taking logarithmic for normaization\nfeature_1 = np.array(np.log(housing_csv['sqft_living'])) # Taking logarithmic for normaization\nfeature_2 = np.array(housing_csv['bedrooms'])","metadata":{"executionTime":124,"lastSuccessfullyExecutedCode":"# Initialize target and features\ntarget = np.array(np.log(housing_csv['price'])) # Taking logarithmic for normaization\nfeature_1 = np.array(np.log(housing_csv['sqft_living'])) # Taking logarithmic for normaization\nfeature_2 = np.array(housing_csv['bedrooms'])"},"cell_type":"code","id":"0eb6e194-9255-4052-81eb-8afa90b7a329","execution_count":42,"outputs":[]},{"source":"# Define multiple linear regression model\ndef mul_linear_regression(params, feature_1=feature_1, feature_2=feature_2):\n    return params[0] + feature_1*params[1] + feature_2*params[2]","metadata":{"executionTime":114,"lastSuccessfullyExecutedCode":"# Define multiple linear regression model\ndef mul_linear_regression(params, feature_1=feature_1, feature_2=feature_2):\n    return params[0] + feature_1*params[1] + feature_2*params[2]"},"cell_type":"code","id":"106799bc-c8d8-4ee9-9841-4d754c82e0ec","execution_count":43,"outputs":[]},{"source":"# Define loss function to calculate MAE\ndef loss_function(params, feature_1=feature_1, feature_2=feature_2, target=target):\n    # Predict using multiple linear regression\n    predictions = mul_linear_regression(params)\n    \n    # Calcuate MAE\n    return keras.losses.mae(target, predictions)","metadata":{"executionTime":105,"lastSuccessfullyExecutedCode":"# Define loss function to calculate MAE\ndef loss_function(params, feature_1=feature_1, feature_2=feature_2, target=target):\n    # Predict using multiple linear regression\n    predictions = mul_linear_regression(params)\n    \n    # Calcuate MAE\n    return keras.losses.mae(target, predictions)"},"cell_type":"code","id":"d5839b26-d6e0-4e93-8281-024bb04c958d","execution_count":44,"outputs":[]},{"source":"# Loss function for params[0.1,0.02,0.5]\nparams = tf.Variable([0.1,0.02,0.5],tf.float32)\nprint(loss_function(params).numpy())","metadata":{"executionTime":205,"lastSuccessfullyExecutedCode":"# Loss function for params[0.1,0.02,0.5]\nparams = tf.Variable([0.1,0.02,0.5],tf.float32)\nprint(loss_function(params).numpy())"},"cell_type":"code","id":"93b2fce9-ab51-4001-b671-eee7af2caddd","execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":"11.111702\n"}]},{"source":"# Optimizing using Adam optimizer\noptim = tf.keras.optimizers.Adam()","metadata":{"executionTime":182,"lastSuccessfullyExecutedCode":"# Optimizing using Adam optimizer\noptim = tf.keras.optimizers.Adam()"},"cell_type":"code","id":"4d9df074-5a95-4d44-b6c9-b307bf3f3587","execution_count":52,"outputs":[]},{"source":"# Minimize the loss funtion using optimizer and evaluate MAE\nparams = tf.Variable([0.1,0.02,0.5],tf.float32)\nfor j in range(100): #Epoch 100\n    optim.minimize(lambda: loss_function(params), var_list=[params])\n    # Print MAE based on given params for every 10th epoch\n    if j%10 == 0:\n        print(\"Params\",params.numpy())\n        print(\"MAE\",loss_function(params).numpy())\n        print()","metadata":{"executionTime":385,"lastSuccessfullyExecutedCode":"# Minimize the loss funtion using optimizer and evaluate MAE\nparams = tf.Variable([0.1,0.02,0.5],tf.float32)\nfor j in range(100): #Epoch 100\n    optim.minimize(lambda: loss_function(params), var_list=[params])\n    # Print MAE based on given params for every 10th epoch\n    if j%10 == 0:\n        print(\"Params\",params.numpy())\n        print(\"MAE\",loss_function(params).numpy())\n        print()"},"cell_type":"code","id":"6be3272f-79b3-413f-a4f7-81c5b09ac677","execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":"Params [0.10198551 0.02198552 0.5019855 ]\nMAE 11.08804\n\nParams [0.13899067 0.05899072 0.53899074]\nMAE 10.647036\n\nParams [0.179733   0.09973308 0.57973313]\nMAE 10.161495\n\nParams [0.2168762  0.1368763  0.61687636]\nMAE 9.718847\n\nParams [0.25021484 0.17021497 0.650215  ]\nMAE 9.321538\n\nParams [0.28045553 0.20045567 0.6804556 ]\nMAE 8.961149\n\nParams [0.30827603 0.22827616 0.70827615]\nMAE 8.629603\n\nParams [0.33419386 0.254194   0.7341941 ]\nMAE 8.320731\n\nParams [0.358587   0.27858713 0.7585873 ]\nMAE 8.030029\n\nParams [0.38173223 0.30173236 0.7817325 ]\nMAE 7.7542005\n\n"}]},{"source":"# Batch Training","metadata":{},"cell_type":"markdown","id":"d05b43e9-79af-4c68-91b5-af48795170fb"},{"source":"# Define the intercept and slope\nintercept = Variable(0.1, tf.float32)\nslope = Variable(0.005, tf.float32)\n\n# Define the model\ndef linear_regression(intercept, slope, features):\n\t# Define the predicted values\n\treturn intercept + features*slope\n\n# Define the loss function\ndef loss_function(intercept, slope, targets, features):\n\t# Define the predicted values\n\tpredictions = linear_regression(intercept, slope, features)\n    \n \t# Define the MAE loss\n\treturn keras.losses.mae(targets, predictions)","metadata":{"executionTime":167,"lastSuccessfullyExecutedCode":"# Define the intercept and slope\nintercept = Variable(0.1, tf.float32)\nslope = Variable(0.005, tf.float32)\n\n# Define the model\ndef linear_regression(intercept, slope, features):\n\t# Define the predicted values\n\treturn intercept + features*slope\n\n# Define the loss function\ndef loss_function(intercept, slope, targets, features):\n\t# Define the predicted values\n\tpredictions = linear_regression(intercept, slope, features)\n    \n \t# Define the MAE loss\n\treturn keras.losses.mae(targets, predictions)"},"cell_type":"code","id":"8138aafe-c46a-4d2e-ab15-a62bb59bff80","execution_count":82,"outputs":[]},{"source":"# Initialize Adam optimizer\nopt = keras.optimizers.Adam()\n\n# Count the batch\ncount = 0 \n\n# Load data in batches\nfor batch in pd.read_csv('datasets/kc_house_data.csv', chunksize=100):\n    \n    # Extract sqft for current batch\n    size_batch = np.array(batch['sqft_lot'], np.float32)\n\n    # Extract the price values for the current batch\n    price_batch = np.array(batch['price'], np.float32)\n    \n    # Batch count\n    count += 1\n    if count%10 == 0:\n        print(\"MSE before minimization of \",count,\" batch:\", loss_function(intercept,\n                                                            slope, \n                                                            price_batch, \n                                                            size_batch).numpy())\n    \n    # Minimization of parameters\n    opt.minimize(lambda: loss_function(intercept, slope, price_batch, size_batch), \n                 var_list=[intercept, slope])\n    \n    if count%10 == 0:\n        print(\"MSE after minimization of \",count, \" batch:\", loss_function(intercept,\n                                                                       slope, \n                                                                       price_batch, \n                                                                       size_batch).numpy())\n        print()\n    ","metadata":{"executionTime":1117,"lastSuccessfullyExecutedCode":"# Initialize Adam optimizer\nopt = keras.optimizers.Adam()\n\n# Count the batch\ncount = 0 \n\n# Load data in batches\nfor batch in pd.read_csv('datasets/kc_house_data.csv', chunksize=100):\n    \n    # Extract sqft for current batch\n    size_batch = np.array(batch['sqft_lot'], np.float32)\n\n    # Extract the price values for the current batch\n    price_batch = np.array(batch['price'], np.float32)\n    \n    # Batch count\n    count += 1\n    if count%10 == 0:\n        print(\"MSE before minimization of \",count,\" batch:\", loss_function(intercept,\n                                                            slope, \n                                                            price_batch, \n                                                            size_batch).numpy())\n    \n    # Minimization of parameters\n    opt.minimize(lambda: loss_function(intercept, slope, price_batch, size_batch), \n                 var_list=[intercept, slope])\n    \n    if count%10 == 0:\n        print(\"MSE after minimization of \",count, \" batch:\", loss_function(intercept,\n                                                                       slope, \n                                                                       price_batch, \n                                                                       size_batch).numpy())\n        print()\n    "},"cell_type":"code","id":"1980b9e4-71ed-4612-8f58-201eaac61fc7","execution_count":86,"outputs":[{"output_type":"stream","name":"stdout","text":"MSE before minimization of  10  batch: 466823.28\nMSE after minimization of  10  batch: 466811.62\n\nMSE before minimization of  20  batch: 507766.3\nMSE after minimization of  20  batch: 507753.72\n\nMSE before minimization of  30  batch: 509910.75\nMSE after minimization of  30  batch: 509888.2\n\nMSE before minimization of  40  batch: 589219.8\nMSE after minimization of  40  batch: 589193.2\n\nMSE before minimization of  50  batch: 550550.56\nMSE after minimization of  50  batch: 550534.2\n\nMSE before minimization of  60  batch: 528731.75\nMSE after minimization of  60  batch: 528717.5\n\nMSE before minimization of  70  batch: 521815.1\nMSE after minimization of  70  batch: 521798.7\n\nMSE before minimization of  80  batch: 546294.94\nMSE after minimization of  80  batch: 546284.7\n\nMSE before minimization of  90  batch: 509868.6\nMSE after minimization of  90  batch: 509855.88\n\nMSE before minimization of  100  batch: 524421.9\nMSE after minimization of  100  batch: 524409.06\n\nMSE before minimization of  110  batch: 536313.3\nMSE after minimization of  110  batch: 536295.44\n\nMSE before minimization of  120  batch: 601132.8\nMSE after minimization of  120  batch: 601112.94\n\nMSE before minimization of  130  batch: 414707.72\nMSE after minimization of  130  batch: 414685.16\n\nMSE before minimization of  140  batch: 562792.3\nMSE after minimization of  140  batch: 562779.56\n\nMSE before minimization of  150  batch: 550211.06\nMSE after minimization of  150  batch: 550194.25\n\nMSE before minimization of  160  batch: 489062.03\nMSE after minimization of  160  batch: 489043.53\n\nMSE before minimization of  170  batch: 509760.7\nMSE after minimization of  170  batch: 509741.28\n\nMSE before minimization of  180  batch: 497082.38\nMSE after minimization of  180  batch: 497073.53\n\nMSE before minimization of  190  batch: 519139.56\nMSE after minimization of  190  batch: 519128.47\n\nMSE before minimization of  200  batch: 607576.06\nMSE after minimization of  200  batch: 607570.4\n\nMSE before minimization of  210  batch: 540388.4\nMSE after minimization of  210  batch: 540382.06\n\n"}]},{"source":"Now while minimizing dataset, we have around 210 iterations because of the chunksize of 100, at every iteration, 100 datapoints are used to minimize. Thus minimization occurs around 210 times for each epoch. Hence, batch training is more powerful. ","metadata":{},"cell_type":"markdown","id":"17d3bc05-9ef8-4dbd-9185-02c14b9f9de9"}],"metadata":{"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}