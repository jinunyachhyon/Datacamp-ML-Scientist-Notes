{"cells":[{"source":"# Fine-tuning your XGBoost model","metadata":{},"cell_type":"markdown","id":"785b6266-2d51-491a-a619-47ad4f1fd5c2"},{"source":"## Tuning the number of boosting rounds","metadata":{},"cell_type":"markdown","id":"728a5136-f396-4776-b256-4d521b3145c5"},{"source":"import pandas as pd\n\nboston = pd.read_csv('datasets/ames_housing_trimmed_processed.csv')\nX,y = boston.iloc[:,:-1],boston.iloc[:,-1]","metadata":{"executionTime":75,"lastSuccessfullyExecutedCode":"import pandas as pd\n\nboston = pd.read_csv('datasets/ames_housing_trimmed_processed.csv')\nX,y = boston.iloc[:,:-1],boston.iloc[:,-1]"},"cell_type":"code","id":"6cec4ef3-e554-40c9-b58b-ce2f1de1da28","execution_count":1,"outputs":[]},{"source":"import xgboost as xgb\n\n# Create the DMatrix: housing_dmatrix\nhousing_dmatrix = xgb.DMatrix(data=X,label=y)\n\n# Create the parameter dictionary for each tree: params \nparams = {\"objective\":\"reg:linear\", \"max_depth\":3}\n\n# Create list of number of boosting rounds\nnum_rounds = [5, 10, 15]\n\n# Empty list to store final round rmse per XGBoost model\nfinal_rmse_per_round = []\n\n# Iterate over num_rounds and build one model per num_boost_round parameter\nfor curr_num_rounds in num_rounds:\n\n    # Perform cross-validation: cv_results\n    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, num_boost_round=curr_num_rounds, metrics=\"rmse\", as_pandas=True, seed=123)\n    \n    # Append final round RMSE\n    final_rmse_per_round.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n\n# Print the resultant DataFrame\nnum_rounds_rmses = list(zip(num_rounds, final_rmse_per_round))\nprint(pd.DataFrame(num_rounds_rmses,columns=[\"num_boosting_rounds\",\"rmse\"]))","metadata":{"executionTime":1026,"lastSuccessfullyExecutedCode":"import xgboost as xgb\n\n# Create the DMatrix: housing_dmatrix\nhousing_dmatrix = xgb.DMatrix(data=X,label=y)\n\n# Create the parameter dictionary for each tree: params \nparams = {\"objective\":\"reg:linear\", \"max_depth\":3}\n\n# Create list of number of boosting rounds\nnum_rounds = [5, 10, 15]\n\n# Empty list to store final round rmse per XGBoost model\nfinal_rmse_per_round = []\n\n# Iterate over num_rounds and build one model per num_boost_round parameter\nfor curr_num_rounds in num_rounds:\n\n    # Perform cross-validation: cv_results\n    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, num_boost_round=curr_num_rounds, metrics=\"rmse\", as_pandas=True, seed=123)\n    \n    # Append final round RMSE\n    final_rmse_per_round.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n\n# Print the resultant DataFrame\nnum_rounds_rmses = list(zip(num_rounds, final_rmse_per_round))\nprint(pd.DataFrame(num_rounds_rmses,columns=[\"num_boosting_rounds\",\"rmse\"]))"},"cell_type":"code","id":"08d7f25d-3201-448b-a53c-d8e2cf3706d4","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":"[17:27:39] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n[17:27:39] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n[17:27:39] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n[17:27:39] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n[17:27:39] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n[17:27:39] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n[17:27:39] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n[17:27:39] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n[17:27:39] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n   num_boosting_rounds          rmse\n0                    5  50903.299752\n1                   10  34774.194090\n2                   15  32895.099185\n"}]},{"source":"**Output:** Increasing the number of boosting rounds decreases the RMSE","metadata":{},"cell_type":"markdown","id":"8bb3e48e-34bc-44e1-ad4e-09b7d0da72bf"},{"source":"## Automated boosting round selection using early_stopping","metadata":{},"cell_type":"markdown","id":"18c9d5db-0234-41fb-b1df-e99d8fc87c90"},{"source":"Instead of attempting to cherry pick the best possible number of boosting rounds, you can very easily have XGBoost automatically select the number of boosting rounds for you within `xgb.cv()`. This is done using a technique called **early stopping**.\n\n- Early stopping works by testing the XGBoost model after every boosting round against a hold-out dataset and stopping the creation of additional boosting rounds (thereby finishing training of the model early) if the hold-out metric (\"rmse\" in our case) does not improve for a given number of rounds.","metadata":{},"cell_type":"markdown","id":"de1026d2-4835-4275-9db1-0d8444f0b39d"},{"source":"# Create your housing DMatrix: housing_dmatrix\nhousing_dmatrix = xgb.DMatrix(data=X, label=y)\n\n# Create the parameter dictionary for each tree: params\nparams = {\"objective\":\"reg:linear\", \"max_depth\":4}\n\n# Perform cross-validation with early stopping: cv_results\ncv_results = xgb.cv(dtrain=housing_dmatrix,\n                    params=params,\n                    nfold=3,\n                    metrics='rmse',\n                    early_stopping_rounds=10, \n                    num_boost_round=100,\n                    as_pandas=True,\n                    seed=123)\n\n# Print cv_results\nprint(cv_results)","metadata":{"executionTime":740,"lastSuccessfullyExecutedCode":"# Create your housing DMatrix: housing_dmatrix\nhousing_dmatrix = xgb.DMatrix(data=X, label=y)\n\n# Create the parameter dictionary for each tree: params\nparams = {\"objective\":\"reg:linear\", \"max_depth\":4}\n\n# Perform cross-validation with early stopping: cv_results\ncv_results = xgb.cv(dtrain=housing_dmatrix,\n                    params=params,\n                    nfold=3,\n                    metrics='rmse',\n                    early_stopping_rounds=10, \n                    num_boost_round=100,\n                    as_pandas=True,\n                    seed=123)\n\n# Print cv_results\nprint(cv_results)"},"cell_type":"code","id":"94623415-fb95-4ae3-8526-7ef2ce48a8c1","execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":"[17:42:10] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n[17:42:10] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n[17:42:10] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n    train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n0     141871.635216      403.633062   142640.653507     705.559723\n1     103057.033818       73.768079   104907.664683     111.117033\n2      75975.967655      253.727043    79262.056654     563.766693\n3      57420.530642      521.658273    61620.137859    1087.693428\n4      44552.956483      544.170426    50437.560906    1846.446643\n5      35763.948865      681.796675    43035.659539    2034.471115\n6      29861.464164      769.571418    38600.880800    2169.796804\n7      25994.675122      756.520639    36071.817710    2109.795408\n8      23306.836299      759.237848    34383.186387    1934.547433\n9      21459.770256      745.624640    33509.140338    1887.375358\n10     20148.721060      749.612186    32916.806725    1850.893437\n11     19215.382607      641.387200    32197.833474    1734.456654\n12     18627.388962      716.256240    31770.852340    1802.154296\n13     17960.695080      557.043324    31482.782172    1779.124406\n14     17559.736640      631.413137    31389.990252    1892.320326\n15     17205.713357      590.171774    31302.883291    1955.165882\n16     16876.571801      703.631953    31234.058914    1880.706205\n17     16597.662170      703.677363    31318.347820    1828.860754\n18     16330.460661      607.274258    31323.634893    1775.909992\n19     16005.972387      520.470815    31204.135450    1739.076237\n20     15814.300847      518.604822    31089.863868    1756.022175\n21     15493.405856      505.616461    31047.997697    1624.673447\n22     15270.734205      502.018639    31056.916210    1668.043691\n23     15086.381896      503.913078    31024.984403    1548.985086\n24     14917.608289      486.206137    30983.685376    1663.131135\n25     14709.589477      449.668262    30989.476981    1686.667218\n26     14457.286251      376.787759    30952.113767    1613.172390\n27     14185.567149      383.102597    31066.901381    1648.534545\n28     13934.066721      473.465580    31095.641882    1709.225578\n29     13749.644941      473.670743    31103.886799    1778.879849\n30     13549.836644      454.898742    30976.084872    1744.514518\n31     13413.484678      399.603422    30938.469354    1746.053330\n32     13275.915700      415.408595    30931.000055    1772.469405\n33     13085.878211      493.792795    30929.056846    1765.541040\n34     12947.181279      517.790033    30890.629160    1786.510472\n35     12846.027264      547.732747    30884.493051    1769.728787\n36     12702.378727      505.523140    30833.542124    1691.002007\n37     12532.244170      508.298300    30856.688154    1771.445485\n38     12384.055037      536.224929    30818.016568    1782.785175\n39     12198.443769      545.165604    30839.393263    1847.326671\n40     12054.583621      508.841802    30776.965294    1912.780332\n41     11897.036784      477.177932    30794.702627    1919.675130\n42     11756.221708      502.992363    30780.956160    1906.820178\n43     11618.846752      519.837483    30783.754746    1951.260120\n44     11484.080227      578.428500    30776.731276    1953.447810\n45     11356.552654      565.368946    30758.543732    1947.454939\n46     11193.557745      552.298986    30729.971937    1985.699239\n47     11071.315547      604.090125    30732.663173    1966.997252\n48     10950.778492      574.862853    30712.241251    1957.750615\n"}]},{"source":"## Common tree tunable parameters\n![image](image.png)\n","metadata":{},"cell_type":"markdown","id":"a31dec2b-e0b5-4385-9cb3-0ce901c5386c"},{"source":"## Linear tunable parameters\n![image-2](image-2.png)\n","metadata":{},"cell_type":"markdown","id":"0c3747da-4288-4e0f-ad19-30d5737dbea3"},{"source":"## Tuning eta","metadata":{},"cell_type":"markdown","id":"f548c450-adf9-4bb7-af4e-153ec2b9f22b"},{"source":"# Create your housing DMatrix: housing_dmatrix\nhousing_dmatrix = xgb.DMatrix(data=X, label=y)\n\n# Create the parameter dictionary for each tree (boosting round)\nparams = {\"objective\":\"reg:linear\", \"max_depth\":3}\n\n# Create list of eta values and empty list to store final round rmse per xgboost model\neta_vals = [0.001, 0.01, 0.1]\nbest_rmse = []\n\n# Systematically vary the eta \nfor curr_val in eta_vals:\n\n    params[\"eta\"] = curr_val\n    \n    # Perform cross-validation: cv_results\n    cv_results = xgb.cv(dtrain=housing_dmatrix,\n                        params=params,\n                        nfold=3,\n                        early_stopping_rounds=5,\n                        num_boost_round=10,\n                        metrics='rmse',\n                        seed=123,\n                        as_pandas=True)\n    \n    \n    \n    # Append the final round rmse to best_rmse\n    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n\n# Print the resultant DataFrame\nprint(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns=[\"eta\",\"best_rmse\"]))","metadata":{"executionTime":364,"lastSuccessfullyExecutedCode":"# Create your housing DMatrix: housing_dmatrix\nhousing_dmatrix = xgb.DMatrix(data=X, label=y)\n\n# Create the parameter dictionary for each tree (boosting round)\nparams = {\"objective\":\"reg:linear\", \"max_depth\":3}\n\n# Create list of eta values and empty list to store final round rmse per xgboost model\neta_vals = [0.001, 0.01, 0.1]\nbest_rmse = []\n\n# Systematically vary the eta \nfor curr_val in eta_vals:\n\n    params[\"eta\"] = curr_val\n    \n    # Perform cross-validation: cv_results\n    cv_results = xgb.cv(dtrain=housing_dmatrix,\n                        params=params,\n                        nfold=3,\n                        early_stopping_rounds=5,\n                        num_boost_round=10,\n                        metrics='rmse',\n                        seed=123,\n                        as_pandas=True)\n    \n    \n    \n    # Append the final round rmse to best_rmse\n    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n\n# Print the resultant DataFrame\nprint(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns=[\"eta\",\"best_rmse\"]))"},"cell_type":"code","id":"42bcb3aa-8ec6-4076-83b9-9e6a9a89d5e4","execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":"[17:57:22] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n[17:57:22] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n[17:57:22] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n[17:57:22] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n[17:57:22] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n[17:57:22] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n[17:57:22] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n[17:57:22] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n[17:57:22] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n     eta      best_rmse\n0  0.001  195736.402543\n1  0.010  179932.183986\n2  0.100   79759.411808\n"}]},{"source":"## Tuning max_depth","metadata":{},"cell_type":"markdown","id":"0a32336d-7266-43b3-aca4-5320116eab60"},{"source":"# Create your housing DMatrix\nhousing_dmatrix = xgb.DMatrix(data=X,label=y)\n\n# Create the parameter dictionary\nparams = {\"objective\":\"reg:linear\"}\n\n# Create list of max_depth values\nmax_depths = [2,5,10,20]\nbest_rmse = []\n\n# Systematically vary the max_depth\nfor curr_val in max_depths:\n\n    params[\"max_depth\"] = curr_val\n    \n    # Perform cross-validation\n    cv_results = xgb.cv(dtrain=housing_dmatrix,\n                        params=params,\n                        nfold=2,\n                        early_stopping_rounds=5,\n                        num_boost_round=10,\n                        metrics='rmse',\n                        seed=123,\n                        as_pandas=True)\n    \n    \n    \n    # Append the final round rmse to best_rmse\n    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n\n# Print the resultant DataFrame\nprint(pd.DataFrame(list(zip(max_depths, best_rmse)),columns=[\"max_depth\",\"best_rmse\"]))","metadata":{"executionTime":442,"lastSuccessfullyExecutedCode":"# Create your housing DMatrix\nhousing_dmatrix = xgb.DMatrix(data=X,label=y)\n\n# Create the parameter dictionary\nparams = {\"objective\":\"reg:linear\"}\n\n# Create list of max_depth values\nmax_depths = [2,5,10,20]\nbest_rmse = []\n\n# Systematically vary the max_depth\nfor curr_val in max_depths:\n\n    params[\"max_depth\"] = curr_val\n    \n    # Perform cross-validation\n    cv_results = xgb.cv(dtrain=housing_dmatrix,\n                        params=params,\n                        nfold=2,\n                        early_stopping_rounds=5,\n                        num_boost_round=10,\n                        metrics='rmse',\n                        seed=123,\n                        as_pandas=True)\n    \n    \n    \n    # Append the final round rmse to best_rmse\n    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n\n# Print the resultant DataFrame\nprint(pd.DataFrame(list(zip(max_depths, best_rmse)),columns=[\"max_depth\",\"best_rmse\"]))"},"cell_type":"code","id":"13606cdf-92eb-455f-b47d-c0d3b58bab35","execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":"[18:01:00] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n[18:01:00] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n[18:01:00] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n[18:01:00] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n[18:01:00] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n[18:01:00] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n[18:01:00] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n[18:01:00] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n   max_depth     best_rmse\n0          2  37957.469464\n1          5  35596.599504\n2         10  36065.547345\n3         20  36739.576068\n"}]},{"source":"## Tuning colsample_bytree","metadata":{},"cell_type":"markdown","id":"ee8ec75a-a979-4122-a281-7b795d172f05"},{"source":"# Create your housing DMatrix\nhousing_dmatrix = xgb.DMatrix(data=X,label=y)\n\n# Create the parameter dictionary\nparams={\"objective\":\"reg:linear\",\"max_depth\":3}\n\n# Create list of hyperparameter values: colsample_bytree_vals\ncolsample_bytree_vals = [0.1,0.5,0.8,1]\nbest_rmse = []\n\n# Systematically vary the hyperparameter value \nfor curr_val in colsample_bytree_vals:\n\n    params['colsample_bytree'] = curr_val\n    \n    # Perform cross-validation\n    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,\n                 num_boost_round=10, early_stopping_rounds=5,\n                 metrics=\"rmse\", as_pandas=True, seed=123)\n    \n    # Append the final round rmse to best_rmse\n    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n\n# Print the resultant DataFrame\nprint(pd.DataFrame(list(zip(colsample_bytree_vals, best_rmse)), columns=[\"colsample_bytree\",\"best_rmse\"]))","metadata":{"executionTime":377,"lastSuccessfullyExecutedCode":"# Create your housing DMatrix\nhousing_dmatrix = xgb.DMatrix(data=X,label=y)\n\n# Create the parameter dictionary\nparams={\"objective\":\"reg:linear\",\"max_depth\":3}\n\n# Create list of hyperparameter values: colsample_bytree_vals\ncolsample_bytree_vals = [0.1,0.5,0.8,1]\nbest_rmse = []\n\n# Systematically vary the hyperparameter value \nfor curr_val in colsample_bytree_vals:\n\n    params['colsample_bytree'] = curr_val\n    \n    # Perform cross-validation\n    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,\n                 num_boost_round=10, early_stopping_rounds=5,\n                 metrics=\"rmse\", as_pandas=True, seed=123)\n    \n    # Append the final round rmse to best_rmse\n    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n\n# Print the resultant DataFrame\nprint(pd.DataFrame(list(zip(colsample_bytree_vals, best_rmse)), columns=[\"colsample_bytree\",\"best_rmse\"]))"},"cell_type":"code","id":"778504ef-ea4b-43c5-8b82-c7d2192dcac1","execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":"[18:03:29] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n[18:03:29] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n[18:03:29] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n[18:03:29] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n[18:03:29] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n[18:03:29] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n[18:03:29] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n[18:03:29] WARNING: ../src/objective/regression_obj.cu:203: reg:linear is now deprecated in favor of reg:squarederror.\n   colsample_bytree     best_rmse\n0               0.1  50033.734626\n1               0.5  35656.185735\n2               0.8  36399.002280\n3               1.0  35836.044343\n"}]},{"source":"## Grid search with XGBoost","metadata":{},"cell_type":"markdown","id":"072d68e1-a477-492f-adc7-ea8f98e91776"},{"source":"from sklearn.model_selection import GridSearchCV\nimport numpy as np\n\n# Create the parameter grid: gbm_param_grid\ngbm_param_grid = {\n    'colsample_bytree': [0.3, 0.7],\n    'n_estimators': [50],\n    'max_depth': [2,5]}\n\n# Instantiate the regressor: gbm\ngbm = xgb.XGBRegressor()\n\n# Perform grid search: grid_mse\ngrid_mse = GridSearchCV(estimator=gbm,\n                        param_grid=gbm_param_grid,\n                        scoring='neg_mean_squared_error',\n                        cv=4,\n                        verbose=1)\n\n\n# Fit grid_mse to the data\ngrid_mse.fit(X,y)\n\n# Print the best parameters and lowest RMSE\nprint(\"Best parameters found: \", grid_mse.best_params_)\nprint(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))","metadata":{"executionTime":1777,"lastSuccessfullyExecutedCode":"from sklearn.model_selection import GridSearchCV\nimport numpy as np\n\n# Create the parameter grid: gbm_param_grid\ngbm_param_grid = {\n    'colsample_bytree': [0.3, 0.7],\n    'n_estimators': [50],\n    'max_depth': [2,5]}\n\n# Instantiate the regressor: gbm\ngbm = xgb.XGBRegressor()\n\n# Perform grid search: grid_mse\ngrid_mse = GridSearchCV(estimator=gbm,\n                        param_grid=gbm_param_grid,\n                        scoring='neg_mean_squared_error',\n                        cv=4,\n                        verbose=1)\n\n\n# Fit grid_mse to the data\ngrid_mse.fit(X,y)\n\n# Print the best parameters and lowest RMSE\nprint(\"Best parameters found: \", grid_mse.best_params_)\nprint(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))"},"cell_type":"code","id":"11d900ea-b780-41e6-b4f5-508123f85496","execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":"Fitting 4 folds for each of 4 candidates, totalling 16 fits\nBest parameters found:  {'colsample_bytree': 0.3, 'max_depth': 5, 'n_estimators': 50}\nLowest RMSE found:  29916.017850830365\n"}]},{"source":"## Random search with XGBoost","metadata":{},"cell_type":"markdown","id":"e36692fe-ecd0-4fc8-a767-3664167a5197"},{"source":"from sklearn.model_selection import RandomizedSearchCV\n\n# Create the parameter grid: gbm_param_grid \ngbm_param_grid = {\n    'n_estimators': [25],\n    'max_depth': range(2, 12)\n}\n\n# Instantiate the regressor: gbm\ngbm = xgb.XGBRegressor(n_estimators=10)\n\n# Perform random search: grid_mse\nrandomized_mse = RandomizedSearchCV(estimator=gbm,\n                                    param_distributions=gbm_param_grid,\n                                    scoring='neg_mean_squared_error',\n                                    n_iter=5,\n                                    cv=4,\n                                    verbose=1)\n\n\n# Fit randomized_mse to the data\nrandomized_mse.fit(X,y)\n\n# Print the best parameters and lowest RMSE\nprint(\"Best parameters found: \", randomized_mse.best_params_)\nprint(\"Lowest RMSE found: \", np.sqrt(np.abs(randomized_mse.best_score_)))","metadata":{"executionTime":1868,"lastSuccessfullyExecutedCode":"from sklearn.model_selection import RandomizedSearchCV\n\n# Create the parameter grid: gbm_param_grid \ngbm_param_grid = {\n    'n_estimators': [25],\n    'max_depth': range(2, 12)\n}\n\n# Instantiate the regressor: gbm\ngbm = xgb.XGBRegressor(n_estimators=10)\n\n# Perform random search: grid_mse\nrandomized_mse = RandomizedSearchCV(estimator=gbm,\n                                    param_distributions=gbm_param_grid,\n                                    scoring='neg_mean_squared_error',\n                                    n_iter=5,\n                                    cv=4,\n                                    verbose=1)\n\n\n# Fit randomized_mse to the data\nrandomized_mse.fit(X,y)\n\n# Print the best parameters and lowest RMSE\nprint(\"Best parameters found: \", randomized_mse.best_params_)\nprint(\"Lowest RMSE found: \", np.sqrt(np.abs(randomized_mse.best_score_)))"},"cell_type":"code","id":"dce63ed8-c6f5-4381-a097-5f4eb497e64a","execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":"Fitting 4 folds for each of 5 candidates, totalling 20 fits\nBest parameters found:  {'n_estimators': 25, 'max_depth': 4}\nLowest RMSE found:  29998.4522530019\n"}]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}