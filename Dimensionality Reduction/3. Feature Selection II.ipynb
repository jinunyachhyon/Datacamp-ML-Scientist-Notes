{"cells":[{"source":"# Feature Selection II - Selecting for Model Accuracy","metadata":{},"cell_type":"markdown","id":"9dd3d688-8783-42a4-a4bf-2877463da449"},{"source":"import pandas as pd\nansur_female = pd.read_csv('datasets/ANSUR_II_FEMALE.csv')\nansur_male = pd.read_csv('datasets/ANSUR_II_MALE.csv')\n\nansur = pd.concat([ansur_female,ansur_male])\nansur.shape","metadata":{"executionTime":267,"lastSuccessfullyExecutedCode":"import pandas as pd\nansur_female = pd.read_csv('datasets/ANSUR_II_FEMALE.csv')\nansur_male = pd.read_csv('datasets/ANSUR_II_MALE.csv')\n\nansur = pd.concat([ansur_female,ansur_male])\nansur.shape"},"cell_type":"code","id":"c9103ca3-663d-4e5c-8683-6ece197db70c","execution_count":1,"outputs":[{"output_type":"execute_result","execution_count":1,"data":{"text/plain":"(6068, 99)"},"metadata":{}}]},{"source":"ansur_filtered = ansur[['Gender','chestdepth','handlength',\n                        'neckcircumference','shoulderlength',\n                       'earlength']]\nansur_filtered.head()","metadata":{"executionTime":137,"lastSuccessfullyExecutedCode":"ansur_filtered = ansur[['Gender','chestdepth','handlength',\n                        'neckcircumference','shoulderlength',\n                       'earlength']]\nansur_filtered.head()"},"cell_type":"code","id":"1bca757e-4bd7-4d49-ac2c-b7fd000444f1","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"application/com.datacamp.data-table.v1+json":{"table":{"schema":{"fields":[{"name":"index","type":"integer"},{"name":"Gender","type":"string"},{"name":"chestdepth","type":"integer"},{"name":"handlength","type":"integer"},{"name":"neckcircumference","type":"integer"},{"name":"shoulderlength","type":"integer"},{"name":"earlength","type":"integer"}],"primaryKey":["index"],"pandas_version":"1.4.0"},"data":[{"index":0,"Gender":"Female","chestdepth":245,"handlength":184,"neckcircumference":335,"shoulderlength":148,"earlength":65},{"index":1,"Gender":"Female","chestdepth":206,"handlength":189,"neckcircumference":302,"shoulderlength":142,"earlength":60},{"index":2,"Gender":"Female","chestdepth":223,"handlength":195,"neckcircumference":325,"shoulderlength":164,"earlength":65},{"index":3,"Gender":"Female","chestdepth":285,"handlength":186,"neckcircumference":357,"shoulderlength":157,"earlength":62},{"index":4,"Gender":"Female","chestdepth":290,"handlength":187,"neckcircumference":340,"shoulderlength":156,"earlength":65}]},"total_rows":5,"truncation_type":null},"text/plain":"   Gender  chestdepth  handlength  neckcircumference  shoulderlength  earlength\n0  Female         245         184                335             148         65\n1  Female         206         189                302             142         60\n2  Female         223         195                325             164         65\n3  Female         285         186                357             157         62\n4  Female         290         187                340             156         65","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Gender</th>\n      <th>chestdepth</th>\n      <th>handlength</th>\n      <th>neckcircumference</th>\n      <th>shoulderlength</th>\n      <th>earlength</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Female</td>\n      <td>245</td>\n      <td>184</td>\n      <td>335</td>\n      <td>148</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Female</td>\n      <td>206</td>\n      <td>189</td>\n      <td>302</td>\n      <td>142</td>\n      <td>60</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Female</td>\n      <td>223</td>\n      <td>195</td>\n      <td>325</td>\n      <td>164</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Female</td>\n      <td>285</td>\n      <td>186</td>\n      <td>357</td>\n      <td>157</td>\n      <td>62</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Female</td>\n      <td>290</td>\n      <td>187</td>\n      <td>340</td>\n      <td>156</td>\n      <td>65</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"source":"# Features\nX = ansur_filtered.drop('Gender',axis=1)\n\n# Target\ny = ansur_filtered['Gender']","metadata":{"executionTime":0,"lastSuccessfullyExecutedCode":"# Features\nX = ansur_filtered.drop('Gender',axis=1)\n\n# Target\ny = ansur_filtered['Gender']"},"cell_type":"code","id":"dd8141bc-c553-4f92-81bd-1a00b96debc0","execution_count":3,"outputs":[]},{"source":"# Pre-processing the data\n\n# Split into train-test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\n# Standardize the data, mean=0,variance=1\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_std = scaler.fit_transform(X_train)","metadata":{"executionTime":237,"lastSuccessfullyExecutedCode":"# Pre-processing the data\n\n# Split into train-test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\n# Standardize the data, mean=0,variance=1\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_std = scaler.fit_transform(X_train)"},"cell_type":"code","id":"6a7abf5a-ee7c-4432-b12e-558588c82555","execution_count":4,"outputs":[]},{"source":"# Creating a logistic regression model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# Create Logistic regression model and fit using standardized data\nlr = LogisticRegression()\nlr.fit(X_train_std, y_train)\n\n# Calculate accuarcy of the model \nX_test_std = scaler.transform(X_test) #<-- first standardized\n\ny_pred = lr.predict(X_test_std)\nprint(accuracy_score(y_test,y_pred))","metadata":{"executionTime":101,"lastSuccessfullyExecutedCode":"# Creating a logistic regression model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# Create Logistic regression model and fit using standardized data\nlr = LogisticRegression()\nlr.fit(X_train_std, y_train)\n\n# Calculate accuarcy of the model \nX_test_std = scaler.transform(X_test) #<-- first standardized\n\ny_pred = lr.predict(X_test_std)\nprint(accuracy_score(y_test,y_pred))"},"cell_type":"code","id":"29cd6d6e-dbde-428f-89f5-9e9a5cd7b837","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":"0.9906644700713894\n"}]},{"source":"# Inspecting the feature coefficients\nprint(lr.coef_)","metadata":{"executionTime":80,"lastSuccessfullyExecutedCode":"# Inspecting the feature coefficients\nprint(lr.coef_)"},"cell_type":"code","id":"b35738f8-ac51-4036-98b1-b28171e3796b","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":"[[-3.08469117  0.02544785  7.66460597  1.33982551  0.67397921]]\n"}]},{"source":"**Note:** These coefficients will be multiplied with the feature values when the model makes a prediction, features with coefficients close to zero will contribute little to the end result.","metadata":{},"cell_type":"markdown","id":"a1318c8c-d9bb-4225-9bb3-9701d2402dd5"},{"source":"print(dict(zip(X.columns, abs(lr.coef_[0]))))","metadata":{"executionTime":167,"lastSuccessfullyExecutedCode":"print(dict(zip(X.columns, abs(lr.coef_[0]))))"},"cell_type":"code","id":"8e93c322-1e5c-484a-9856-1543bcd7d944","execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":"{'chestdepth': 3.0846911680204783, 'handlength': 0.02544785454550054, 'neckcircumference': 7.664605966495018, 'shoulderlength': 1.3398255121268206, 'earlength': 0.6739792057328612}\n"}]},{"source":"# Dropping feature that contribute little to the model\nX.drop('handlength',axis=1, inplace=True)\n\n# Recalculating the accuarcy after dropping a feature\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\nlr = LogisticRegression()\nlr.fit(scaler.fit_transform(X_train), y_train)\n\nprint(accuracy_score(y_test, lr.predict(scaler.transform(X_test))))","metadata":{"executionTime":185,"lastSuccessfullyExecutedCode":"# Dropping feature that contribute little to the model\nX.drop('handlength',axis=1, inplace=True)\n\n# Recalculating the accuarcy after dropping a feature\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\nlr = LogisticRegression()\nlr.fit(scaler.fit_transform(X_train), y_train)\n\nprint(accuracy_score(y_test, lr.predict(scaler.transform(X_test))))"},"cell_type":"code","id":"4e38cfb5-c94d-4c9d-a13a-d47666d358f4","execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":"0.9879187259747392\n"}]},{"source":"Thus, increased accuracy and decreased model complexity. To repeat this step recursively we have RFE (Recursive Feature Elimination).","metadata":{},"cell_type":"markdown","id":"cd630a3f-2428-4f73-a3ef-93f932d41523"},{"source":"## Recursive Feature Elimination\n- Feature selection algorithm that can be wrapped around any model that produces feature coefficients or feature importance values.\n- We can pass it the model we want to use and the number of features we want to select. \n-  While fitting to our data it will repeat a process where it first fits the internal model and then drops the feature with the weakest coefficient.\n-  It will keep doing this until the desired number of features is reached.","metadata":{},"cell_type":"markdown","id":"502bc7ce-1c6f-42e3-b080-9151cc4909d6"},{"source":"# Features\nX = ansur_filtered.drop('Gender',axis=1)\n\n# Target\ny = ansur_filtered['Gender']","metadata":{"executionTime":0,"lastSuccessfullyExecutedCode":"# Features\nX = ansur_filtered.drop('Gender',axis=1)\n\n# Target\ny = ansur_filtered['Gender']"},"cell_type":"code","id":"940ffe42-ff14-40dc-9205-ac4ce9f64c37","execution_count":9,"outputs":[]},{"source":"# Pre-processing the data\n\n# Split into train-test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\n# Standardize the data, mean=0,variance=1\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_std = scaler.fit_transform(X_train)","metadata":{"executionTime":0,"lastSuccessfullyExecutedCode":"# Pre-processing the data\n\n# Split into train-test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\n# Standardize the data, mean=0,variance=1\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_std = scaler.fit_transform(X_train)"},"cell_type":"code","id":"140505f6-3ff7-4c24-8728-068e5ac41ca9","execution_count":10,"outputs":[]},{"source":"# RFE\nfrom sklearn.feature_selection import RFE\n\n# Instantiate RFE\nrfe = RFE(estimator=LogisticRegression(), n_features_to_select=2, verbose=1)\n\n# Fit the model\nrfe.fit(X_train_std, y_train)","metadata":{"executionTime":330,"lastSuccessfullyExecutedCode":"# RFE\nfrom sklearn.feature_selection import RFE\n\n# Instantiate RFE\nrfe = RFE(estimator=LogisticRegression(), n_features_to_select=2, verbose=1)\n\n# Fit the model\nrfe.fit(X_train_std, y_train)"},"cell_type":"code","id":"e9461fc3-4896-46e0-aff3-12bff2d7f52c","execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":"Fitting estimator with 5 features.\nFitting estimator with 4 features.\nFitting estimator with 3 features.\n"},{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"RFE(estimator=LogisticRegression(), n_features_to_select=2, verbose=1)","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RFE(estimator=LogisticRegression(), n_features_to_select=2, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RFE</label><div class=\"sk-toggleable__content\"><pre>RFE(estimator=LogisticRegression(), n_features_to_select=2, verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div>"},"metadata":{}}]},{"source":"# Inspecting the RFE results\nX.columns[rfe.support_]","metadata":{"executionTime":245,"lastSuccessfullyExecutedCode":"# Inspecting the RFE results\nX.columns[rfe.support_]"},"cell_type":"code","id":"22a565c5-b728-4be4-bcba-14c981671c5f","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"Index(['chestdepth', 'neckcircumference'], dtype='object')"},"metadata":{}}]},{"source":"# See in which iteration a feature was dropped\ndict(zip(X.columns, rfe.ranking_))","metadata":{"executionTime":108,"lastSuccessfullyExecutedCode":"# See in which iteration a feature was dropped\ndict(zip(X.columns, rfe.ranking_))"},"cell_type":"code","id":"624c2452-58f6-460a-8af2-b241f0a340cc","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"{'chestdepth': 1,\n 'handlength': 4,\n 'neckcircumference': 1,\n 'shoulderlength': 2,\n 'earlength': 3}"},"metadata":{}}]},{"source":"# Check accuracy from the two remaining feature\nX_test_std = scaler.transform(X_test)\n\naccuracy_score(y_test, rfe.predict(X_test_std))","metadata":{"executionTime":111,"lastSuccessfullyExecutedCode":"# Check accuracy from the two remaining feature\nX_test_std = scaler.transform(X_test)\n\naccuracy_score(y_test, rfe.predict(X_test_std))"},"cell_type":"code","id":"3fe8ac4d-58ef-4161-a330-7c0d564f3707","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"0.9824272377814388"},"metadata":{}}]},{"source":"# Tree-based feature selection","metadata":{},"cell_type":"markdown","id":"d2ed7e7e-526e-477a-8b64-0bcfc720b3f4"},{"source":"![image-3](image-3.png)\n\nRandom Forest is one of such models that performs feature selection by design to avoid overfitting. \n- It pass different, random, subsets of features to a number of decision trees.","metadata":{},"cell_type":"markdown","id":"e2051aa5-6a3b-418c-a675-d3cdcf8e1549"},{"source":"import pandas as pd\nansur_female = pd.read_csv('datasets/ANSUR_II_FEMALE.csv')\nansur_male = pd.read_csv('datasets/ANSUR_II_MALE.csv')\n\nansur = pd.concat([ansur_female,ansur_male])\nansur.shape","metadata":{"executionTime":109,"lastSuccessfullyExecutedCode":"import pandas as pd\nansur_female = pd.read_csv('datasets/ANSUR_II_FEMALE.csv')\nansur_male = pd.read_csv('datasets/ANSUR_II_MALE.csv')\n\nansur = pd.concat([ansur_female,ansur_male])\nansur.shape"},"cell_type":"code","id":"4feb352f-8bc2-4ed4-b020-e35f34dbd634","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"(6068, 99)"},"metadata":{}}]},{"source":"ansur.drop(['Branch','Component','BMI_class','Height_class'],\n          axis=1,\n          inplace=True)","metadata":{"executionTime":0,"lastSuccessfullyExecutedCode":"ansur.drop(['Branch','Component','BMI_class','Height_class'],\n          axis=1,\n          inplace=True)"},"cell_type":"code","id":"3eba67c1-22bb-4cbd-97b3-b46a274203bf","execution_count":16,"outputs":[]},{"source":"# Features\nX = ansur.drop('Gender',axis=1)\n\n# Target\ny = ansur['Gender']","metadata":{"executionTime":0,"lastSuccessfullyExecutedCode":"# Features\nX = ansur.drop('Gender',axis=1)\n\n# Target\ny = ansur['Gender']"},"cell_type":"code","id":"10ea2563-76b5-4a59-94e7-ea07b2e4dbec","execution_count":17,"outputs":[]},{"source":"# Split into train-test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","metadata":{"executionTime":0,"lastSuccessfullyExecutedCode":"# Split into train-test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"},"cell_type":"code","id":"9f1f5d1b-3f62-4ed1-90e2-2298a867b5a7","execution_count":18,"outputs":[]},{"source":"# Random forest classifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\nrf = RandomForestClassifier()\n\nrf.fit(X_train, y_train)\n\nprint(accuracy_score(y_test, rf.predict(X_test)))","metadata":{"executionTime":161,"lastSuccessfullyExecutedCode":"# Random forest classifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\nrf = RandomForestClassifier()\n\nrf.fit(X_train, y_train)\n\nprint(accuracy_score(y_test, rf.predict(X_test)))"},"cell_type":"code","id":"fb02e5a0-8c22-48c2-827e-b40abbb15c51","execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":"0.9906644700713894\n"}]},{"source":"Being able to get such high accuracy means it managed to escape the curse of dimensionality and didn't overfit on the many features in the training set.","metadata":{},"cell_type":"markdown","id":"01b58f2f-70b2-45ff-97ca-6b7a6812d431"},{"source":"# Feature importance values\nrf.feature_importances_","metadata":{"executionTime":49,"lastSuccessfullyExecutedCode":"# Feature importance values\nrf.feature_importances_"},"cell_type":"code","id":"ac05ec70-1de5-4a10-86fb-52b0bce2afe8","execution_count":20,"outputs":[{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"array([0.00158209, 0.00078468, 0.00060322, 0.0007228 , 0.00077227,\n       0.01361131, 0.01930108, 0.08249253, 0.00316029, 0.0073802 ,\n       0.01569823, 0.02589116, 0.00061053, 0.00968738, 0.00149896,\n       0.01546283, 0.00245461, 0.00064102, 0.00224862, 0.00174543,\n       0.00137302, 0.02060868, 0.0007312 , 0.00114741, 0.00752685,\n       0.03379426, 0.0004317 , 0.00570454, 0.00074169, 0.00092596,\n       0.00115428, 0.00244007, 0.00043242, 0.00320249, 0.00134764,\n       0.00765255, 0.00069702, 0.05595191, 0.00744972, 0.00075592,\n       0.00072762, 0.03614581, 0.08194956, 0.00069755, 0.00129565,\n       0.00099696, 0.00070611, 0.01995327, 0.00045674, 0.0131407 ,\n       0.02666373, 0.00090233, 0.00061568, 0.00307083, 0.00414214,\n       0.00059269, 0.00033574, 0.00088505, 0.00549695, 0.00599081,\n       0.00124363, 0.10899823, 0.05932448, 0.00517493, 0.00028992,\n       0.00551621, 0.00509178, 0.09318428, 0.00797017, 0.00087411,\n       0.00565232, 0.02054258, 0.0006657 , 0.00143432, 0.00907028,\n       0.00111834, 0.00805488, 0.0015084 , 0.0034877 , 0.00062206,\n       0.00070301, 0.00068826, 0.00135894, 0.02157975, 0.00169625,\n       0.00112368, 0.00174648, 0.00074338, 0.00070636, 0.04604359,\n       0.00057685, 0.00243703, 0.01195469, 0.00363295])"},"metadata":{}}]},{"source":"An advantage of these feature importance values over coefficients is that they are comparable between features by default, since they always sum up to one.","metadata":{},"cell_type":"markdown","id":"17637d22-408b-4d48-aed4-482ea355160b"},{"source":"# Feature importance as a feature selector\nmask = rf.feature_importances_ > 0.1\nprint(mask)\n\nX_reduced = X.loc[:,mask]\nprint(X_reduced.columns)","metadata":{"executionTime":48,"lastSuccessfullyExecutedCode":"# Feature importance as a feature selector\nmask = rf.feature_importances_ > 0.1\nprint(mask)\n\nX_reduced = X.loc[:,mask]\nprint(X_reduced.columns)"},"cell_type":"code","id":"39cdef9f-55d1-4f67-9912-90918be56c74","execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":"[False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False  True False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False]\nIndex(['neckcircumference'], dtype='object')\n"}]},{"source":"# RFE with random forests\nfrom sklearn.feature_selection import RFE\n\n# Model instantiate\nrfe = RFE(estimator=RandomForestClassifier(),\n         n_features_to_select = 6,\n         step = 10, #<-- at each step 10 least imp. features are dropped\n         verbose = 1)\n\n# Fit the model\nrfe.fit(X_train, y_train)\n\n# Remaining features\nprint(X.columns[rfe.support_])","metadata":{"executionTime":8500,"lastSuccessfullyExecutedCode":"# RFE with random forests\nfrom sklearn.feature_selection import RFE\n\n# Model instantiate\nrfe = RFE(estimator=RandomForestClassifier(),\n         n_features_to_select = 6,\n         step = 10, #<-- at each step 10 least imp. features are dropped\n         verbose = 1)\n\n# Fit the model\nrfe.fit(X_train, y_train)\n\n# Remaining features\nprint(X.columns[rfe.support_])"},"cell_type":"code","id":"3585baa7-4e95-4a56-9ae0-1e8f5360bf7a","execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":"Fitting estimator with 94 features.\nFitting estimator with 84 features.\nFitting estimator with 74 features.\nFitting estimator with 64 features.\nFitting estimator with 54 features.\nFitting estimator with 44 features.\nFitting estimator with 34 features.\nFitting estimator with 24 features.\nFitting estimator with 14 features.\nIndex(['biacromialbreadth', 'handcircumference', 'neckcircumference',\n       'neckcircumferencebase', 'shouldercircumference', 'wristcircumference'],\n      dtype='object')\n"}]},{"source":"# accuracy calculation\naccuracy_score(y_test, rfe.predict(X_test))","metadata":{"executionTime":44,"lastSuccessfullyExecutedCode":"# accuracy calculation\naccuracy_score(y_test, rfe.predict(X_test))"},"cell_type":"code","id":"dd4aaae3-2f4c-4e54-a9b7-e1f8efe7777c","execution_count":23,"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"0.9654036243822076"},"metadata":{}}]},{"source":"# Regularized linear regression\n![image-4](image-4.png)\n","metadata":{},"cell_type":"markdown","id":"75ed68b7-c17c-4dc3-906c-f1dfb97db837"},{"source":"ansur_male.drop(['Branch','Component','BMI_class','Height_class','Gender'],\n          axis=1,\n          inplace=True)","metadata":{"executionTime":162,"lastSuccessfullyExecutedCode":"ansur_male.drop(['Branch','Component','BMI_class','Height_class','Gender'],\n          axis=1,\n          inplace=True)"},"cell_type":"code","id":"8f65c920-e15e-4813-9b88-8ecaa5117e5f","execution_count":24,"outputs":[]},{"source":"ansur_male.shape","metadata":{"executionTime":45,"lastSuccessfullyExecutedCode":"ansur_male.shape"},"cell_type":"code","id":"7794e88c-df35-4513-b9d6-bde7ecd7f38f","execution_count":25,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"(4082, 94)"},"metadata":{}}]},{"source":"X = ansur_male.drop('BMI',axis=1)\ny = ansur_male['BMI']","metadata":{"executionTime":0,"lastSuccessfullyExecutedCode":"X = ansur_male.drop('BMI',axis=1)\ny = ansur_male['BMI']"},"cell_type":"code","id":"8684f43d-e5e6-4482-baa1-327b8b4b37ab","execution_count":26,"outputs":[]},{"source":"from sklearn.linear_model import Lasso\n\n# Set the test size to 30% to get a 70-30% train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n# Fit the scaler on the training features and transform these in one go\nX_train_std = scaler.fit_transform(X_train)\n\n# Create the Lasso model\nla = Lasso()\n\n# Fit it to the standardized training data\nla.fit(X_train_std, y_train)","metadata":{"executionTime":41,"lastSuccessfullyExecutedCode":"from sklearn.linear_model import Lasso\n\n# Set the test size to 30% to get a 70-30% train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n# Fit the scaler on the training features and transform these in one go\nX_train_std = scaler.fit_transform(X_train)\n\n# Create the Lasso model\nla = Lasso()\n\n# Fit it to the standardized training data\nla.fit(X_train_std, y_train)"},"cell_type":"code","id":"0b907bdb-8690-4bb5-8253-00c8f4317940","execution_count":27,"outputs":[{"output_type":"execute_result","execution_count":27,"data":{"text/plain":"Lasso()","text/html":"<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Lasso()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Lasso</label><div class=\"sk-toggleable__content\"><pre>Lasso()</pre></div></div></div></div></div>"},"metadata":{}}]},{"source":"# Transform the test set with the pre-fitted scaler\nX_test_std = scaler.transform(X_test)\n\n# Calculate the coefficient of determination (R squared) on X_test_std\nr_squared = la.score(X_test_std, y_test)\nprint(f\"The model can predict {r_squared:.1%} of the variance in the test set.\")","metadata":{"executionTime":31,"lastSuccessfullyExecutedCode":"# Transform the test set with the pre-fitted scaler\nX_test_std = scaler.transform(X_test)\n\n# Calculate the coefficient of determination (R squared) on X_test_std\nr_squared = la.score(X_test_std, y_test)\nprint(f\"The model can predict {r_squared:.1%} of the variance in the test set.\")"},"cell_type":"code","id":"77d4fac8-3254-4e84-b284-1439ead555d0","execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":"The model can predict 84.7% of the variance in the test set.\n"}]},{"source":"# Feature coeff. (higher the value, higher the importance of the feature)\nla.coef_","metadata":{"executionTime":83,"lastSuccessfullyExecutedCode":"# Feature coeff. (higher the value, higher the importance of the feature)\nla.coef_"},"cell_type":"code","id":"2f4e399a-94d2-4b6c-975b-213a905fa308","execution_count":29,"outputs":[{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"array([ 0.09198472, -0.        , -0.        ,  0.        , -0.        ,\n        0.        , -0.        ,  0.        ,  0.26349677,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.30062148, -0.        , -0.        , -0.        ,\n        0.06019966, -0.        ,  0.        ,  0.84028524,  0.        ,\n       -0.        , -0.        ,  0.        , -0.        ,  0.        ,\n        0.        , -0.        ,  0.        , -0.        ,  0.        ,\n       -0.        , -0.        ,  0.        ,  0.        , -0.        ,\n       -0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        , -0.        ,  0.        ,  0.        ,  0.        ,\n       -0.        , -0.        , -0.        , -0.        ,  0.        ,\n        0.        ,  0.01276255,  0.        , -0.        , -0.        ,\n       -0.        , -0.        ,  0.        , -0.        , -0.        ,\n       -0.        , -0.        , -0.        , -0.        , -0.        ,\n       -0.        ,  0.75354765,  0.        , -0.        , -0.        ,\n       -0.        , -0.        ,  0.        ,  0.        ,  0.        ,\n        0.4317104 ,  0.26286887,  0.        , -0.        ,  0.        ,\n       -0.        ,  0.        , -0.        ])"},"metadata":{}}]},{"source":"# Create a list that has True values when coefficients equal 0\nzero_coef = la.coef_ == 0 \nprint(zero_coef)\n\n# Calculate how many features have a zero coefficient\nn_ignored = sum(zero_coef)\nprint(f\"The model has ignored {n_ignored} out of {len(la.coef_)} features.\")","metadata":{"executionTime":29,"lastSuccessfullyExecutedCode":"# Create a list that has True values when coefficients equal 0\nzero_coef = la.coef_ == 0 \nprint(zero_coef)\n\n# Calculate how many features have a zero coefficient\nn_ignored = sum(zero_coef)\nprint(f\"The model has ignored {n_ignored} out of {len(la.coef_)} features.\")"},"cell_type":"code","id":"4dc4fb96-fdeb-42a1-9b88-a56cd64660ef","execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":"[False  True  True  True  True  True  True  True False  True  True  True\n  True  True  True  True False  True  True  True False  True  True False\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True False  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True False  True  True  True  True  True  True  True\n  True False False  True  True  True  True  True  True]\nThe model has ignored 84 out of 93 features.\n"}]},{"source":"# Lasso Regression with alpha parameter, which shrinks least important features but doen't peanlize the important feature much\nla = Lasso(alpha=0.1, random_state=0)\n\n# Fits the model and calculates performance stats\nla.fit(X_train_std, y_train)\nr_squared = la.score(X_test_std, y_test)\nn_ignored_features = sum(la.coef_ == 0)\n\n# Print peformance stats \nprint(f\"The model can predict {r_squared:.1%} of the variance in the test set.\")\nprint(f\"{n_ignored_features} out of {len(la.coef_)} features were ignored.\")","metadata":{"executionTime":37,"lastSuccessfullyExecutedCode":"# Lasso Regression with alpha parameter, which shrinks least important features but doen't peanlize the important feature much\nla = Lasso(alpha=0.1, random_state=0)\n\n# Fits the model and calculates performance stats\nla.fit(X_train_std, y_train)\nr_squared = la.score(X_test_std, y_test)\nn_ignored_features = sum(la.coef_ == 0)\n\n# Print peformance stats \nprint(f\"The model can predict {r_squared:.1%} of the variance in the test set.\")\nprint(f\"{n_ignored_features} out of {len(la.coef_)} features were ignored.\")"},"cell_type":"code","id":"1d507812-0350-47ec-a69e-0c22c443d547","execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":"The model can predict 99.2% of the variance in the test set.\n75 out of 93 features were ignored.\n"}]},{"source":"# Creating a LassoCV regressor\n- `LassoCV()` regressor automatically tunes the regularization strength (alpha value) using Cross-Validation.","metadata":{},"cell_type":"markdown","id":"1031e610-0375-4b15-90da-c54dfd28bb83"},{"source":"from sklearn.linear_model import LassoCV\n\n# Create and fit the LassoCV model on the training set\nlcv = LassoCV()\nlcv.fit(X_train, y_train)\nprint(f'Optimal alpha = {lcv.alpha_:.3f}')\n\n# Calculate R squared on the test set\nr_squared = lcv.score(X_test, y_test)\nprint(f'The model explains {r_squared:.1%} of the test set variance')\n\n# Create a mask for coefficients not equal to zero\nlcv_mask = (lcv.coef_ != 0)\nprint(f'{sum(lcv_mask)} features out of {len(lcv_mask)} selected')","metadata":{"executionTime":1144,"lastSuccessfullyExecutedCode":"from sklearn.linear_model import LassoCV\n\n# Create and fit the LassoCV model on the training set\nlcv = LassoCV()\nlcv.fit(X_train, y_train)\nprint(f'Optimal alpha = {lcv.alpha_:.3f}')\n\n# Calculate R squared on the test set\nr_squared = lcv.score(X_test, y_test)\nprint(f'The model explains {r_squared:.1%} of the test set variance')\n\n# Create a mask for coefficients not equal to zero\nlcv_mask = (lcv.coef_ != 0)\nprint(f'{sum(lcv_mask)} features out of {len(lcv_mask)} selected')"},"cell_type":"code","id":"edd2f5ec-2526-429c-a920-f225c0c1033f","execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":"Optimal alpha = 0.404\nThe model explains 99.1% of the test set variance\n38 features out of 93 selected\n"}]},{"source":"# Combining feature selectors","metadata":{},"cell_type":"markdown","id":"008c8a50-21bb-43c8-a858-2a42c1185270"},{"source":"### Using GradientBoostingRegressor","metadata":{},"cell_type":"markdown","id":"153d9636-4efe-4f26-b262-7b670321a693"},{"source":"from sklearn.feature_selection import RFE\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n# Select 10 features with RFE on a GradientBoostingRegressor, drop 3 features on each step\nrfe_gb = RFE(estimator=GradientBoostingRegressor(), \n             n_features_to_select=10, step=3, verbose=1)\nrfe_gb.fit(X_train, y_train)\n\n# Calculate the R squared on the test set\nr_squared = rfe_gb.score(X_test,y_test)\nprint(f'The model can explain {r_squared:.1%} of the variance in the test set')","metadata":{"executionTime":60399,"lastSuccessfullyExecutedCode":"from sklearn.feature_selection import RFE\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n# Select 10 features with RFE on a GradientBoostingRegressor, drop 3 features on each step\nrfe_gb = RFE(estimator=GradientBoostingRegressor(), \n             n_features_to_select=10, step=3, verbose=1)\nrfe_gb.fit(X_train, y_train)\n\n# Calculate the R squared on the test set\nr_squared = rfe_gb.score(X_test,y_test)\nprint(f'The model can explain {r_squared:.1%} of the variance in the test set')"},"cell_type":"code","id":"18c9c496-ca69-4260-8886-8ab5d523d3f2","execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":"Fitting estimator with 93 features.\nFitting estimator with 90 features.\nFitting estimator with 87 features.\nFitting estimator with 84 features.\nFitting estimator with 81 features.\nFitting estimator with 78 features.\nFitting estimator with 75 features.\nFitting estimator with 72 features.\nFitting estimator with 69 features.\nFitting estimator with 66 features.\nFitting estimator with 63 features.\nFitting estimator with 60 features.\nFitting estimator with 57 features.\nFitting estimator with 54 features.\nFitting estimator with 51 features.\nFitting estimator with 48 features.\nFitting estimator with 45 features.\nFitting estimator with 42 features.\nFitting estimator with 39 features.\nFitting estimator with 36 features.\nFitting estimator with 33 features.\nFitting estimator with 30 features.\nFitting estimator with 27 features.\nFitting estimator with 24 features.\nFitting estimator with 21 features.\nFitting estimator with 18 features.\nFitting estimator with 15 features.\nFitting estimator with 12 features.\nThe model can explain 97.7% of the variance in the test set\n"}]},{"source":"# Mask --> True:The corresponding feature is selected, otherwise not selected\ngb_mask = rfe_gb.support_","metadata":{"executionTime":70,"lastSuccessfullyExecutedCode":"# Mask --> True:The corresponding feature is selected, otherwise not selected\ngb_mask = rfe_gb.support_","executionCancelledAt":1683956768583},"cell_type":"code","id":"96ab4766-17ed-4445-bdfc-e7c415642f51","execution_count":37,"outputs":[]},{"source":"# Inspecting RFE result in GradientBoostingRegressor \nX.columns[rfe_gb.support_]","metadata":{"executionTime":54,"lastSuccessfullyExecutedCode":"# Inspecting RFE result in GradientBoostingRegressor \nX.columns[rfe_gb.support_]"},"cell_type":"code","id":"3441e726-32c9-45c1-85e0-33fa3b344679","execution_count":38,"outputs":[{"output_type":"execute_result","execution_count":38,"data":{"text/plain":"Index(['bicepscircumferenceflexed', 'buttockcircumference', 'buttockdepth',\n       'calfcircumference', 'chestcircumference', 'neckcircumference',\n       'poplitealheight', 'thighcircumference', 'waistcircumference',\n       'stature_m'],\n      dtype='object')"},"metadata":{}}]},{"source":"### Using RandomForestRegressor","metadata":{},"cell_type":"markdown","id":"c4049725-d58b-4616-b781-123911147e5d"},{"source":"from sklearn.feature_selection import RFE\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Select 10 features with RFE on a RandomForestRegressor, drop 3 features on each step\nrfe_rf = RFE(estimator=RandomForestRegressor(), \n             n_features_to_select=10, step=3, verbose=1)\nrfe_rf.fit(X_train, y_train)\n\n# Calculate the R squared on the test set\nr_squared = rfe_rf.score(X_test, y_test)\nprint(f'The model can explain {r_squared:.1%} of the variance in the test set')\n\n# Assign the support array to rf_mask\nrf_mask = rfe_rf.support_","metadata":{"executionTime":143362,"lastSuccessfullyExecutedCode":"from sklearn.feature_selection import RFE\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Select 10 features with RFE on a RandomForestRegressor, drop 3 features on each step\nrfe_rf = RFE(estimator=RandomForestRegressor(), \n             n_features_to_select=10, step=3, verbose=1)\nrfe_rf.fit(X_train, y_train)\n\n# Calculate the R squared on the test set\nr_squared = rfe_rf.score(X_test, y_test)\nprint(f'The model can explain {r_squared:.1%} of the variance in the test set')\n\n# Assign the support array to rf_mask\nrf_mask = rfe_rf.support_","executionCancelledAt":1683956768581},"cell_type":"code","id":"a29c3311-c168-4009-8613-442b07c216b3","execution_count":39,"outputs":[]},{"source":"# Inspecting RFE result in RandomForestRegressor\nX.columns[rfe_rf.support_]","metadata":{"executionTime":66,"lastSuccessfullyExecutedCode":"# Inspecting RFE result in RandomForestRegressor\nX.columns[rfe_rf.support_]"},"cell_type":"code","id":"d007feec-4e35-4b38-9ceb-478f589180b1","execution_count":40,"outputs":[{"output_type":"execute_result","execution_count":40,"data":{"text/plain":"Index(['axillaheight', 'bicepscircumferenceflexed', 'buttockdepth',\n       'chestcircumference', 'poplitealheight', 'thighcircumference',\n       'waistcircumference', 'waistheightomphalion', 'weight_kg', 'stature_m'],\n      dtype='object')"},"metadata":{}}]},{"source":"## Combining 3 feature selectors\n- Combining the votes of the 3 models built earlier, to decide which features are important into a meta mask. We'll then use this mask to reduce dimensionality and see how a simple linear regressor performs on the reduced dataset.","metadata":{},"cell_type":"markdown","id":"1019d9d8-a4d8-4dc2-9ab9-181d916221a4"},{"source":"import numpy as np\n\n# Sum the votes of the three models\nvotes = np.sum([lcv_mask, rf_mask, gb_mask],axis=0)\nvotes","metadata":{"executionTime":110,"lastSuccessfullyExecutedCode":"import numpy as np\n\n# Sum the votes of the three models\nvotes = np.sum([lcv_mask, rf_mask, gb_mask],axis=0)\nvotes"},"cell_type":"code","id":"2a777d78-9c72-41a9-9758-454818420b47","execution_count":45,"outputs":[{"output_type":"execute_result","execution_count":45,"data":{"text/plain":"array([1, 0, 0, 0, 2, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 2, 3, 1, 0, 0, 2, 1,\n       0, 3, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n       0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 1, 0, 3,\n       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 3, 1, 1, 1, 0, 0, 1, 0, 0, 3, 1, 0,\n       2, 0, 0, 2, 2])"},"metadata":{}}]},{"source":"# Create a mask for features selected by all 3 models\nmeta_mask = votes == 3\nmeta_mask","metadata":{"executionTime":80,"lastSuccessfullyExecutedCode":"# Create a mask for features selected by all 3 models\nmeta_mask = votes == 3\nmeta_mask"},"cell_type":"code","id":"9b8362cb-fde8-4f5a-b4e7-f938a3874120","execution_count":46,"outputs":[{"output_type":"execute_result","execution_count":46,"data":{"text/plain":"array([False, False, False, False, False, False, False, False,  True,\n       False, False, False, False, False, False, False,  True, False,\n       False, False, False, False, False,  True, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False,  True, False, False, False, False, False, False,\n       False, False, False, False,  True, False, False, False, False,\n       False, False, False, False,  True, False, False, False, False,\n       False, False, False])"},"metadata":{}}]},{"source":"# Apply the dimensionality reduction on X\nX_reduced = X.iloc[:,meta_mask]\nprint(X_reduced.columns)","metadata":{"executionTime":117,"lastSuccessfullyExecutedCode":"# Apply the dimensionality reduction on X\nX_reduced = X.iloc[:,meta_mask]\nprint(X_reduced.columns)"},"cell_type":"code","id":"a14950f1-044b-4d70-b41e-3c148567f420","execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":"Index(['bicepscircumferenceflexed', 'buttockdepth', 'chestcircumference',\n       'poplitealheight', 'thighcircumference', 'waistcircumference'],\n      dtype='object')\n"}]},{"source":"from sklearn.linear_model import LinearRegression\n\n# Instantiate the model\nlm = LinearRegression()\n\n# Plug the reduced dataset into a linear regression pipeline\nX_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.3, random_state=0)\nlm.fit(scaler.fit_transform(X_train), y_train)\nr_squared = lm.score(scaler.transform(X_test), y_test)\nprint(f'The model can explain {r_squared:.1%} of the variance in the test set using {len(lm.coef_)} features.')","metadata":{"executionTime":99,"lastSuccessfullyExecutedCode":"from sklearn.linear_model import LinearRegression\n\n# Instantiate the model\nlm = LinearRegression()\n\n# Plug the reduced dataset into a linear regression pipeline\nX_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.3, random_state=0)\nlm.fit(scaler.fit_transform(X_train), y_train)\nr_squared = lm.score(scaler.transform(X_test), y_test)\nprint(f'The model can explain {r_squared:.1%} of the variance in the test set using {len(lm.coef_)} features.')"},"cell_type":"code","id":"70ab9ec8-fbd0-48b0-b2b1-8e624fd67665","execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":"The model can explain 95.7% of the variance in the test set using 6 features.\n"}]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}