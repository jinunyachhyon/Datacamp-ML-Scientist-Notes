{"cells":[{"source":"# Word counts with bag-of-words\n- Basic method for finding topics in a text\n- Need to first create tokens using tokenization\n- ... and then count up all the tokens\n- **The more frequent a word, the more important it might be**","metadata":{},"cell_type":"markdown","id":"4d2fc11d-14a8-4161-bc30-d97f0b82432f"},{"source":"import zipfile\n\nzip_path = \"datasets/News%20articles.zip\"  \ndestination_path = \"datasets/news_article\" \n\n# Open the zip file\nwith zipfile.ZipFile(zip_path, 'r') as zip_ref:\n    # Extract all contents of the zip file to the destination folder\n    zip_ref.extractall(destination_path)\n","metadata":{"executionTime":70,"lastSuccessfullyExecutedCode":"import zipfile\n\nzip_path = \"datasets/News%20articles.zip\"  \ndestination_path = \"datasets/news_article\" \n\n# Open the zip file\nwith zipfile.ZipFile(zip_path, 'r') as zip_ref:\n    # Extract all contents of the zip file to the destination folder\n    zip_ref.extractall(destination_path)\n"},"cell_type":"code","id":"1986613b-be01-4eef-b9aa-7a87197c82a6","execution_count":2,"outputs":[]},{"source":"file_path = \"datasets/news_article/News articles/articles.txt\"  \n\n# Open the text file in read mode\nwith open(file_path, 'r') as file:\n    # Read the contents of the file\n    article = file.read()","metadata":{"executionTime":89,"lastSuccessfullyExecutedCode":"file_path = \"datasets/news_article/News articles/articles.txt\"  \n\n# Open the text file in read mode\nwith open(file_path, 'r') as file:\n    # Read the contents of the file\n    article = file.read()"},"cell_type":"code","id":"3ef67ae0-908a-4d53-8407-a753636ba346","execution_count":1,"outputs":[]},{"source":"import nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\n# Import Counter\nfrom collections import Counter\n\n# Tokenize the article: tokens\ntokens = word_tokenize(article)\n\n# Convert the tokens into lowercase: lower_tokens\nlower_tokens = [t.lower() for t in tokens]\n\n# Create a Counter with the lowercase tokens: bow_simple\nbow_simple = Counter(lower_tokens)\n\n# Print the 10 most common tokens\nprint(bow_simple.most_common(10))","metadata":{"executionTime":838,"lastSuccessfullyExecutedCode":"import nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\n# Import Counter\nfrom collections import Counter\n\n# Tokenize the article: tokens\ntokens = word_tokenize(article)\n\n# Convert the tokens into lowercase: lower_tokens\nlower_tokens = [t.lower() for t in tokens]\n\n# Create a Counter with the lowercase tokens: bow_simple\nbow_simple = Counter(lower_tokens)\n\n# Print the 10 most common tokens\nprint(bow_simple.most_common(10))"},"cell_type":"code","id":"90cbd023-3ce8-4b6b-b4e1-69c568c59124","execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":"[nltk_data] Downloading package punkt to /home/repl/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n"},{"output_type":"stream","name":"stdout","text":"[('the', 274), (',', 269), ('.', 189), ('to', 131), ('of', 119), ('a', 100), ('in', 99), ('and', 80), ('that', 67), ('â€™', 54)]\n"}]},{"source":"# Simple text preprocessing\n- Tokenization to create bag-of-words\n- Lowercasing words\n- Lemmatization/Stemming -> shorten words to their root stems\n- Removing stop words, punctuations, unwanted tokens","metadata":{},"cell_type":"markdown","id":"dbe12b42-a66c-4857-9bd0-82a804e267a4"},{"source":"- Remove stop words and non-alphabetic characters, lemmatize, and perform a new bag-of-words on your cleaned text.\n\nYou start with the same tokens you created in the last exercise: `lower_tokens`. ","metadata":{},"cell_type":"markdown","id":"ec43ef4b-c2d7-4fa0-b2d3-fe962df1b3cd"},{"source":"file_path = 'datasets/english_stopwords.txt'\n\n# Open the text file in read mode\nwith open(file_path, 'r') as file:\n    # Read the contents of the file\n    english_stop = file.read()","metadata":{"executionTime":88,"lastSuccessfullyExecutedCode":"file_path = 'datasets/english_stopwords.txt'\n\n# Open the text file in read mode\nwith open(file_path, 'r') as file:\n    # Read the contents of the file\n    english_stop = file.read()"},"cell_type":"code","id":"5611f6d9-854d-4f51-b222-9317162f42b2","execution_count":3,"outputs":[]},{"source":"# Creating a list of stop_words\nenglish_stop = english_stop.split('\\n')","metadata":{"executionTime":114,"lastSuccessfullyExecutedCode":"# Creating a list of stop_words\nenglish_stop = english_stop.split('\\n')"},"cell_type":"code","id":"acb84527-78e5-4888-9335-a13ba0ce0805","execution_count":4,"outputs":[]},{"source":"# Import WordNetLemmatizer and download WordNet resource\nimport nltk\nnltk.download('omw-1.4')\nnltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\n\n# Retain alphabetic words: alpha_only\nalpha_only = [t for t in lower_tokens if t.isalpha()]\n\n# Remove all stop words: no_stops\nno_stops = [t for t in alpha_only if t not in english_stop]\n\n# Instantiate the WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\n\n# Lemmatize all tokens into a new list: lemmatized\nlemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n\n# Create the bag-of-words: bow\nbow = Counter(lemmatized)\n\n# Print the 10 most common tokens\nprint(bow.most_common(10))","metadata":{"executionTime":1378,"lastSuccessfullyExecutedCode":"# Import WordNetLemmatizer and download WordNet resource\nimport nltk\nnltk.download('omw-1.4')\nnltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\n\n# Retain alphabetic words: alpha_only\nalpha_only = [t for t in lower_tokens if t.isalpha()]\n\n# Remove all stop words: no_stops\nno_stops = [t for t in alpha_only if t not in english_stop]\n\n# Instantiate the WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\n\n# Lemmatize all tokens into a new list: lemmatized\nlemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n\n# Create the bag-of-words: bow\nbow = Counter(lemmatized)\n\n# Print the 10 most common tokens\nprint(bow.most_common(10))"},"cell_type":"code","id":"ae2af0cb-736b-478b-b4f3-dd6bcdd1cd93","execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":"[nltk_data] Downloading package omw-1.4 to /home/repl/nltk_data...\n[nltk_data] Downloading package wordnet to /home/repl/nltk_data...\n"},{"output_type":"stream","name":"stdout","text":"[('said', 29), ('robot', 28), ('population', 22), ('news', 19), ('human', 16), ('growth', 16), ('fake', 15), ('country', 14), ('united', 14), ('machine', 13)]\n"}]},{"source":"# Introduction to gensim\n- Popular open source NLP library\n- Uses top academic model to perform complex tasks:\n    - Building document or word vectors\n    - Performing topic identification or document comparison","metadata":{},"cell_type":"markdown","id":"84a78cbf-da1a-44b6-99ed-d53809cf7f8d"},{"source":"## Word Vectors\n\n![image-2](image-2.png)\n\nA word embedding or vector is trained from a larger corpus and is a multi-dimensional representation of a word or document. You can think of it as a multi-dimensional array normally with sparse features (lots of zeros and some ones). With these vectors, we can then see relationships among the words or documents based on how near or far they are and also what similar comparisons we find. For example, in this graphic we can see that the vector operation king minus queen is approximately equal to man minus woman. Or that Spain is to Madrid as Italy is to Rome. ","metadata":{},"cell_type":"markdown","id":"b9671387-eba7-45bb-ae8e-faa09008845e"},{"source":"# Creating the list of documents\narticle = article.split(\"\\n\\n\\n\")","metadata":{"executionTime":97,"lastSuccessfullyExecutedCode":"# Creating the list of documents\narticle = article.split(\"\\n\\n\\n\")"},"cell_type":"code","id":"b084ad9b-1160-490d-b749-17b7ea83a826","execution_count":6,"outputs":[]},{"source":"import nltk\nfrom nltk.tokenize import word_tokenize\n\ntokenized_documents = []\n\n# Tokenize each document in the list\nfor doc in article:\n    # Lowercase each doc\n    lower_tokens = word_tokenize(doc.lower())\n    \n    # Retain alphabetic words: alpha_only\n    alpha_only = [t for t in lower_tokens if t.isalpha()]\n    \n    # Remove all stop words: no_stops\n    no_stops = [t for t in alpha_only if t not in english_stop]\n    \n    # Instantiate the WordNetLemmatizer\n    wordnet_lemmatizer = WordNetLemmatizer()\n    \n    # Lemmatize all tokens into a new list: lemmatized\n    lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n    \n    tokenized_documents.append(lemmatized)","metadata":{"executionTime":164,"lastSuccessfullyExecutedCode":"import nltk\nfrom nltk.tokenize import word_tokenize\n\ntokenized_documents = []\n\n# Tokenize each document in the list\nfor doc in article:\n    # Lowercase each doc\n    lower_tokens = word_tokenize(doc.lower())\n    \n    # Retain alphabetic words: alpha_only\n    alpha_only = [t for t in lower_tokens if t.isalpha()]\n    \n    # Remove all stop words: no_stops\n    no_stops = [t for t in alpha_only if t not in english_stop]\n    \n    # Instantiate the WordNetLemmatizer\n    wordnet_lemmatizer = WordNetLemmatizer()\n    \n    # Lemmatize all tokens into a new list: lemmatized\n    lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n    \n    tokenized_documents.append(lemmatized)"},"cell_type":"code","id":"b0ea6534-21b4-4ead-9d1d-36c3ea2a5644","execution_count":40,"outputs":[]},{"source":"# First 3 tokenized documents in the list\ntokenized_documents[:3]","metadata":{"executionTime":186,"lastSuccessfullyExecutedCode":"# First 3 tokenized documents in the list\ntokenized_documents[:3]"},"cell_type":"code","id":"4b0b964e-e5b8-44e3-9aab-07905831efeb","execution_count":41,"outputs":[{"output_type":"execute_result","execution_count":41,"data":{"text/plain":"[['copyright',\n  'epa',\n  'image',\n  'caption',\n  'uber',\n  'criticised',\n  'many',\n  'time',\n  'way',\n  'run',\n  'business'],\n ['firm', 'uber', 'facing', 'criminal', 'investigation', 'u', 'government'],\n ['scrutiny',\n  'started',\n  'firm',\n  'accused',\n  'using',\n  'secret',\n  'software',\n  'let',\n  'operate',\n  'region',\n  'banned',\n  'restricted']]"},"metadata":{}}]},{"source":"# Import Dictionary\nfrom gensim.corpora.dictionary import Dictionary\n\n# Create a Dictionary from the articles: dictionary\ndictionary = Dictionary(tokenized_documents)\ndictionary","metadata":{"executionTime":90,"lastSuccessfullyExecutedCode":"# Import Dictionary\nfrom gensim.corpora.dictionary import Dictionary\n\n# Create a Dictionary from the articles: dictionary\ndictionary = Dictionary(tokenized_documents)\ndictionary"},"cell_type":"code","id":"c3764a02-19a0-4008-bdf8-f241f135b727","execution_count":42,"outputs":[{"output_type":"execute_result","execution_count":42,"data":{"text/plain":"<gensim.corpora.dictionary.Dictionary at 0x7f536d606b50>"},"metadata":{}}]},{"source":"**Dictionary** class create a mapping with an id for each token, which is the beginning of corpus (helps with NLP). Now we can represent whole document using just a list of their token ids and how often these tokens appear in each document.","metadata":{},"cell_type":"markdown","id":"fab470bb-530c-4b9e-badf-aa1b3fa09abb"},{"source":"dictionary.token2id #<-- token2id attribute to look, tokens with their respective ids","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false},"executionTime":341,"lastSuccessfullyExecutedCode":"dictionary.token2id #<-- token2id attribute to look, tokens with their respective ids"},"cell_type":"code","id":"4eb07ef5-86c5-4ef7-bb68-8bee00d278ca","execution_count":43,"outputs":[{"output_type":"execute_result","execution_count":43,"data":{"text/plain":"{'business': 0,\n 'caption': 1,\n 'copyright': 2,\n 'criticised': 3,\n 'epa': 4,\n 'image': 5,\n 'many': 6,\n 'run': 7,\n 'time': 8,\n 'uber': 9,\n 'way': 10,\n 'criminal': 11,\n 'facing': 12,\n 'firm': 13,\n 'government': 14,\n 'investigation': 15,\n 'u': 16,\n 'accused': 17,\n 'banned': 18,\n 'let': 19,\n 'operate': 20,\n 'region': 21,\n 'restricted': 22,\n 'scrutiny': 23,\n 'secret': 24,\n 'software': 25,\n 'started': 26,\n 'using': 27,\n 'called': 28,\n 'greyball': 29,\n 'helped': 30,\n 'identify': 31,\n 'official': 32,\n 'running': 33,\n 'seeking': 34,\n 'service': 35,\n 'stop': 36,\n 'agency': 37,\n 'comment': 38,\n 'declined': 39,\n 'news': 40,\n 'reported': 41,\n 'reuters': 42,\n 'spokesman': 43,\n 'approval': 44,\n 'area': 45,\n 'claimed': 46,\n 'including': 47,\n 'oregon': 48,\n 'portland': 49,\n 'ride': 50,\n 'several': 51,\n 'still': 52,\n 'used': 53,\n 'blocked': 54,\n 'booking': 55,\n 'bid': 56,\n 'company': 57,\n 'driver': 58,\n 'illegally': 59,\n 'operating': 60,\n 'passenger': 61,\n 'posed': 62,\n 'prove': 63,\n 'regulation': 64,\n 'transport': 65,\n 'worked': 66,\n 'april': 67,\n 'city': 68,\n 'exceedingly': 69,\n 'granted': 70,\n 'last': 71,\n 'letter': 72,\n 'permission': 73,\n 'regulator': 74,\n 'said': 75,\n 'sent': 76,\n 'since': 77,\n 'sparingly': 78,\n 'week': 79,\n 'added': 80,\n 'blog': 81,\n 'defended': 82,\n 'earlier': 83,\n 'fraud': 84,\n 'harm': 85,\n 'legitimate': 86,\n 'limit': 87,\n 'new': 88,\n 'protect': 89,\n 'request': 90,\n 'revealed': 91,\n 'saying': 92,\n 'use': 93,\n 'work': 94,\n 'year': 95,\n 'york': 96,\n 'act': 97,\n 'clear': 98,\n 'face': 99,\n 'find': 100,\n 'sanction': 101,\n 'alexa': 102,\n 'car': 103,\n 'come': 104,\n 'complaining': 105,\n 'criticism': 106,\n 'current': 107,\n 'currently': 108,\n 'device': 109,\n 'difficult': 110,\n 'faced': 111,\n 'fighting': 112,\n 'found': 113,\n 'front': 114,\n 'hype': 115,\n 'inquiry': 116,\n 'lawsuit': 117,\n 'linguistics': 118,\n 'medium': 119,\n 'people': 120,\n 'rachael': 121,\n 'researcher': 122,\n 'rise': 123,\n 'siri': 124,\n 'social': 125,\n 'tatman': 126,\n 'technology': 127,\n 'understand': 128,\n 'accent': 129,\n 'conference': 130,\n 'country': 131,\n 'especially': 132,\n 'francisco': 133,\n 'like': 134,\n 'live': 135,\n 'regional': 136,\n 'san': 137,\n 'south': 138,\n 'true': 139,\n 'zealand': 140,\n 'delight': 141,\n 'experience': 142,\n 'going': 143,\n 'opposite': 144,\n 'twitter': 145,\n 'upset': 146,\n 'user': 147,\n 'whatever': 148,\n 'yelling': 149,\n 'adjacent': 150,\n 'advancing': 151,\n 'artificial': 152,\n 'assistant': 153,\n 'candidate': 154,\n 'central': 155,\n 'deep': 156,\n 'department': 157,\n 'doctoral': 158,\n 'drew': 159,\n 'friday': 160,\n 'hotel': 161,\n 'intelligence': 162,\n 'learning': 163,\n 'machine': 164,\n 'one': 165,\n 'park': 166,\n 'representing': 167,\n 'robotics': 168,\n 'speaker': 169,\n 'summit': 170,\n 'university': 171,\n 'virtual': 172,\n 'washington': 173,\n 'working': 174,\n 'wrapped': 175,\n 'already': 176,\n 'amazon': 177,\n 'apple': 178,\n 'beyond': 179,\n 'consumer': 180,\n 'echo': 181,\n 'horizon': 182,\n 'include': 183,\n 'lot': 184,\n 'mobile': 185,\n 'moving': 186,\n 'phone': 187,\n 'program': 188,\n 'discern': 189,\n 'done': 190,\n 'example': 191,\n 'fall': 192,\n 'human': 193,\n 'learn': 194,\n 'little': 195,\n 'pattern': 196,\n 'quickly': 197,\n 'recognition': 198,\n 'sentence': 199,\n 'short': 200,\n 'speech': 201,\n 'two': 202,\n 'account': 203,\n 'factor': 204,\n 'gender': 205,\n 'met': 206,\n 'person': 207,\n 'previously': 208,\n 'someone': 209,\n 'take': 210,\n 'talking': 211,\n 'whether': 212,\n 'automatic': 213,\n 'california': 214,\n 'captioning': 215,\n 'error': 216,\n 'examined': 217,\n 'language': 218,\n 'lived': 219,\n 'showed': 220,\n 'southern': 221,\n 'spoken': 222,\n 'text': 223,\n 'translate': 224,\n 'translation': 225,\n 'word': 226,\n 'youtube': 227,\n 'deal': 228,\n 'demographic': 229,\n 'largest': 230,\n 'market': 231,\n 'problem': 232,\n 'reaching': 233,\n 'state': 234,\n 'united': 235,\n 'ceo': 236,\n 'could': 237,\n 'crucial': 238,\n 'econtext': 239,\n 'future': 240,\n 'revenue': 241,\n 'scarr': 242,\n 'search': 243,\n 'serve': 244,\n 'stephen': 245,\n 'trying': 246,\n 'developer': 247,\n 'important': 248,\n 'percent': 249,\n 'radar': 250,\n 'really': 251,\n 'told': 252,\n 'voice': 253,\n 'audio': 254,\n 'boy': 255,\n 'challenge': 256,\n 'channel': 257,\n 'instead': 258,\n 'misunderstanding': 259,\n 'offering': 260,\n 'play': 261,\n 'porn': 262,\n 'recent': 263,\n 'song': 264,\n 'video': 265,\n 'young': 266,\n 'alonso': 267,\n 'animation': 268,\n 'animator': 269,\n 'audience': 270,\n 'connection': 271,\n 'create': 272,\n 'cue': 273,\n 'director': 274,\n 'emotional': 275,\n 'martinez': 276,\n 'pixar': 277,\n 'robot': 278,\n 'studio': 279,\n 'technical': 280,\n 'touched': 281,\n 'admirable': 282,\n 'ask': 283,\n 'character': 284,\n 'developed': 285,\n 'emeryville': 286,\n 'faceless': 287,\n 'generic': 288,\n 'hit': 289,\n 'inside': 290,\n 'make': 291,\n 'movie': 292,\n 'need': 293,\n 'thing': 294,\n 'think': 295,\n 'thinking': 296,\n 'value': 297,\n 'wish': 298,\n 'collaborate': 299,\n 'corina': 300,\n 'easily': 301,\n 'elena': 302,\n 'grigore': 303,\n 'lab': 304,\n 'manufacturing': 305,\n 'perform': 306,\n 'repetitive': 307,\n 'specialized': 308,\n 'task': 309,\n 'trained': 310,\n 'yale': 311,\n 'advance': 312,\n 'assembling': 313,\n 'chair': 314,\n 'changing': 315,\n 'complex': 316,\n 'help': 317,\n 'ikea': 318,\n 'maddening': 319,\n 'played': 320,\n 'slowly': 321,\n 'achieve': 322,\n 'action': 323,\n 'anything': 324,\n 'anytime': 325,\n 'capability': 326,\n 'common': 327,\n 'creativity': 328,\n 'dynamic': 329,\n 'environment': 330,\n 'flexibility': 331,\n 'getting': 332,\n 'hard': 333,\n 'necessary': 334,\n 'physical': 335,\n 'point': 336,\n 'related': 337,\n 'replaced': 338,\n 'require': 339,\n 'sense': 340,\n 'soon': 341,\n 'type': 342,\n 'algorithm': 343,\n 'automatically': 344,\n 'benny': 345,\n 'bevangelista': 346,\n 'britain': 347,\n 'chronicle': 348,\n 'chroniclebenny': 349,\n 'coming': 350,\n 'computer': 351,\n 'data': 352,\n 'election': 353,\n 'electorate': 354,\n 'email': 355,\n 'evangelista': 356,\n 'fake': 357,\n 'faster': 358,\n 'followed': 359,\n 'french': 360,\n 'germany': 361,\n 'giant': 362,\n 'group': 363,\n 'head': 364,\n 'may': 365,\n 'month': 366,\n 'online': 367,\n 'poll': 368,\n 'presidential': 369,\n 'ream': 370,\n 'round': 371,\n 'scientist': 372,\n 'second': 373,\n 'sophisticated': 374,\n 'spot': 375,\n 'staff': 376,\n 'tech': 377,\n 'traditional': 378,\n 'vote': 379,\n 'writer': 380,\n 'advertisement': 381,\n 'continue': 382,\n 'main': 383,\n 'reading': 384,\n 'story': 385,\n 'across': 386,\n 'anger': 387,\n 'caused': 388,\n 'confusion': 389,\n 'counter': 390,\n 'digital': 391,\n 'europe': 392,\n 'expand': 393,\n 'expert': 394,\n 'facebook': 395,\n 'false': 396,\n 'goal': 397,\n 'much': 398,\n 'november': 399,\n 'outright': 400,\n 'report': 401,\n 'routinely': 402,\n 'say': 403,\n 'spread': 404,\n 'tool': 405,\n 'wildfire': 406,\n 'claire': 407,\n 'debunk': 408,\n 'draft': 409,\n 'first': 410,\n 'hand': 411,\n 'heavy': 412,\n 'impossible': 413,\n 'lifting': 414,\n 'misinformation': 415,\n 'newsroom': 416,\n 'nonprofit': 417,\n 'organization': 418,\n 'research': 419,\n 'strategy': 420,\n 'teamed': 421,\n 'wardle': 422,\n 'american': 423,\n 'issue': 424,\n 'making': 425,\n 'merely': 426,\n 'mutated': 427,\n 'replicate': 428,\n 'response': 429,\n 'tried': 430,\n 'advertising': 431,\n 'claim': 432,\n 'different': 433,\n 'european': 434,\n 'le': 435,\n 'mean': 436,\n 'prevalent': 437,\n 'profit': 438,\n 'set': 439,\n 'shared': 440,\n 'site': 441,\n 'smaller': 442,\n 'context': 443,\n 'far': 444,\n 'often': 445,\n 'rare': 446,\n 'real': 447,\n 'relatively': 448,\n 'russian': 449,\n 'sputnik': 450,\n 'taking': 451,\n 'well': 452,\n 'also': 453,\n 'analyst': 454,\n 'around': 455,\n 'combat': 456,\n 'enough': 457,\n 'forthcoming': 458,\n 'swirling': 459,\n 'threat': 460,\n 'worry': 461,\n 'actor': 462,\n 'amount': 463,\n 'amplify': 464,\n 'bring': 465,\n 'center': 466,\n 'coder': 467,\n 'communication': 468,\n 'excellence': 469,\n 'hackathon': 470,\n 'hold': 471,\n 'increased': 472,\n 'janis': 473,\n 'latvia': 474,\n 'local': 475,\n 'mainstream': 476,\n 'nato': 477,\n 'potential': 478,\n 'riga': 479,\n 'sarts': 480,\n 'solution': 481,\n 'specific': 482,\n 'strategic': 483,\n 'tank': 484,\n 'trend': 485,\n 'view': 486,\n 'biggest': 487,\n 'call': 488,\n 'combating': 489,\n 'focused': 490,\n 'google': 491,\n 'player': 492,\n 'address': 493,\n 'analysis': 494,\n 'bit': 495,\n 'box': 496,\n 'clicking': 497,\n 'daily': 498,\n 'editor': 499,\n 'exclusive': 500,\n 'industry': 501,\n 'interested': 502,\n 'invalid': 503,\n 'keep': 504,\n 'later': 505,\n 'latest': 506,\n 'must': 507,\n 'newsletter': 508,\n 'occasional': 509,\n 'occurred': 510,\n 'offer': 511,\n 'please': 512,\n 'plus': 513,\n 'product': 514,\n 'receive': 515,\n 'reporter': 516,\n 'select': 517,\n 'sign': 518,\n 'silicon': 519,\n 'special': 520,\n 'subscribe': 521,\n 'subscribing': 522,\n 'thank': 523,\n 'try': 524,\n 'update': 525,\n 'updated': 526,\n 'valley': 527,\n 'verify': 528,\n 'ahead': 529,\n 'approximately': 530,\n 'dutch': 531,\n 'fraction': 532,\n 'france': 533,\n 'introduced': 534,\n 'march': 535,\n 'million': 536,\n 'removed': 537,\n 'role': 538,\n 'small': 539,\n 'spreading': 540,\n 'photo': 541,\n 'embraced': 542,\n 'everyone': 543,\n 'though': 544,\n 'balked': 545,\n 'clamp': 546,\n 'effort': 547,\n 'fine': 548,\n 'german': 549,\n 'hate': 550,\n 'hefty': 551,\n 'instance': 552,\n 'lawmaker': 553,\n 'mulling': 554,\n 'network': 555,\n 'participating': 556,\n 'publisher': 557,\n 'responsibility': 558,\n 'aimed': 559,\n 'almost': 560,\n 'british': 561,\n 'funded': 562,\n 'includes': 563,\n 'june': 564,\n 'looking': 565,\n 'parliamentary': 566,\n 'potentially': 567,\n 'project': 568,\n 'support': 569,\n 'similarly': 570,\n 'topic': 571,\n 'track': 572,\n 'trending': 573,\n 'arabia': 574,\n 'arguably': 575,\n 'campaign': 576,\n 'david': 577,\n 'debunked': 578,\n 'dieudonnÃ©': 579,\n 'emmanuel': 580,\n 'february': 581,\n 'figure': 582,\n 'funding': 583,\n 'leading': 584,\n 'macron': 585,\n 'saudi': 586,\n 'answer': 587,\n 'complicated': 588,\n 'easy': 589,\n 'something': 590,\n 'beast': 591,\n 'big': 592,\n 'driven': 593,\n 'academic': 594,\n 'analyzed': 595,\n 'chavalarias': 596,\n 'created': 597,\n 'helping': 598,\n 'journalist': 599,\n 'message': 600,\n 'review': 601,\n 'carnegie': 602,\n 'challenged': 603,\n 'dean': 604,\n 'distinguish': 605,\n 'follower': 606,\n 'mellon': 607,\n 'pittsburgh': 608,\n 'pomerleau': 609,\n 'anyone': 610,\n 'delip': 611,\n 'early': 612,\n 'former': 613,\n 'meet': 614,\n 'offered': 615,\n 'prize': 616,\n 'rao': 617,\n 'requirement': 618,\n 'signed': 619,\n 'team': 620,\n 'world': 621,\n 'able': 622,\n 'accurately': 623,\n 'article': 624,\n 'certain': 625,\n 'college': 626,\n 'combination': 627,\n 'database': 628,\n 'end': 629,\n 'existing': 630,\n 'expertise': 631,\n 'hope': 632,\n 'independent': 633,\n 'predict': 634,\n 'programmer': 635,\n 'rival': 636,\n 'veracity': 637,\n 'verified': 638,\n 'content': 639,\n 'expects': 640,\n 'move': 641,\n 'multimedia': 642,\n 'next': 643,\n 'share': 644,\n 'toward': 645,\n 'want': 646,\n 'worldwide': 647,\n 'approach': 648,\n 'decidedly': 649,\n 'rush': 650,\n 'within': 651,\n 'combine': 652,\n 'contribute': 653,\n 'crowdfunding': 654,\n 'founder': 655,\n 'jimmy': 656,\n 'professional': 657,\n 'recently': 658,\n 'similar': 659,\n 'volunteer': 660,\n 'wale': 661,\n 'wikipedia': 662,\n 'would': 663,\n 'based': 664,\n 'choose': 665,\n 'community': 666,\n 'effect': 667,\n 'inspired': 668,\n 'interest': 669,\n 'part': 670,\n 'paying': 671,\n 'relying': 672,\n 'reporting': 673,\n 'subject': 674,\n 'subscriber': 675,\n 'technique': 676,\n 'wikitribune': 677,\n 'wizardry': 678,\n 'adapted': 679,\n 'author': 680,\n 'behind': 681,\n 'change': 682,\n 'chief': 683,\n 'cover': 684,\n 'essay': 685,\n 'force': 686,\n 'get': 687,\n 'global': 688,\n 'hire': 689,\n 'impetus': 690,\n 'investment': 691,\n 'management': 692,\n 'morgan': 693,\n 'nation': 694,\n 'ruchir': 695,\n 'sharma': 696,\n 'stanley': 697,\n 'strategist': 698,\n 'cafe': 699,\n 'chilli': 700,\n 'cleaned': 701,\n 'collect': 702,\n 'dish': 703,\n 'nonya': 704,\n 'padi': 705,\n 'singapore': 706,\n 'su': 707,\n 'alarming': 708,\n 'appear': 709,\n 'billion': 710,\n 'border': 711,\n 'cause': 712,\n 'chorus': 713,\n 'exploding': 714,\n 'fear': 715,\n 'food': 716,\n 'forecast': 717,\n 'growth': 718,\n 'humanity': 719,\n 'hungry': 720,\n 'inspire': 721,\n 'invoking': 722,\n 'leaving': 723,\n 'nearly': 724,\n 'number': 725,\n 'obsolete': 726,\n 'others': 727,\n 'outstrip': 728,\n 'overpopulation': 729,\n 'planet': 730,\n 'population': 731,\n 'populist': 732,\n 'prompt': 733,\n 'rapidly': 734,\n 'rising': 735,\n 'shut': 736,\n 'slam': 737,\n 'specter': 738,\n 'supply': 739,\n 'tide': 740,\n 'tirade': 741,\n 'warning': 742,\n 'worker': 743,\n 'danger': 744,\n 'economy': 745,\n 'lie': 746,\n 'likely': 747,\n 'long': 748,\n 'revile': 749,\n 'treasure': 750,\n 'age': 751,\n 'among': 752,\n 'assumes': 753,\n 'boosting': 754,\n 'care': 755,\n 'catastrophic': 756,\n 'child': 757,\n 'chile': 758,\n 'china': 759,\n 'crowd': 760,\n 'decline': 761,\n 'easiest': 762,\n 'economic': 763,\n 'entering': 764,\n 'fewer': 765,\n 'half': 766,\n 'health': 767,\n 'increase': 768,\n 'living': 769,\n 'longer': 770,\n 'matter': 771,\n 'medicine': 772,\n 'owing': 773,\n 'peak': 774,\n 'poised': 775,\n 'rapid': 776,\n 'rate': 777,\n 'retirement': 778,\n 'slowdown': 779,\n 'slumped': 780,\n 'sound': 781,\n 'toxic': 782,\n 'woman': 783,\n 'according': 784,\n 'adding': 785,\n 'aging': 786,\n 'average': 787,\n 'book': 788,\n 'calculation': 789,\n 'clearest': 790,\n 'crisis': 791,\n 'decade': 792,\n 'dramatically': 793,\n 'dropping': 794,\n 'equation': 795,\n 'estimate': 796,\n 'even': 797,\n 'explanation': 798,\n 'fallen': 799,\n 'farmbots': 800,\n 'farmer': 801,\n 'fast': 802,\n 'fill': 803,\n 'financial': 804,\n 'grow': 805,\n 'hostility': 806,\n 'immigrant': 807,\n 'increasing': 808,\n 'labor': 809,\n 'left': 810,\n 'output': 811,\n 'pace': 812,\n 'particularly': 813,\n 'past': 814,\n 'per': 815,\n 'performed': 816,\n 'postwar': 817,\n 'productivity': 818,\n 'recovery': 819,\n 'retiring': 820,\n 'roughly': 821,\n 'side': 822,\n 'simple': 823,\n 'slid': 824,\n 'slow': 825,\n 'void': 826,\n 'acting': 827,\n 'drag': 828,\n 'economist': 829,\n 'emerging': 830,\n 'fact': 831,\n 'however': 832,\n 'industrial': 833,\n 'japan': 834,\n 'korea': 835,\n 'overlooked': 836,\n 'peaked': 837,\n 'shortage': 838,\n 'unfolding': 839,\n 'widely': 840,\n 'worrying': 841,\n 'alarmed': 842,\n 'asked': 843,\n 'automation': 844,\n 'began': 845,\n 'beijing': 846,\n 'companion': 847,\n 'consider': 848,\n 'daniel': 849,\n 'development': 850,\n 'dinner': 851,\n 'elderly': 852,\n 'evercore': 853,\n 'expected': 854,\n 'foreseeable': 855,\n 'four': 856,\n 'involved': 857,\n 'isi': 858,\n 'job': 859,\n 'kahneman': 860,\n 'lose': 861,\n 'negative': 862,\n 'nobel': 863,\n 'record': 864,\n 'responded': 865,\n 'result': 866,\n 'show': 867,\n 'stage': 868,\n 'study': 869,\n 'subsidy': 870,\n 'turning': 871,\n 'twice': 872,\n 'wonder': 873,\n 'authority': 874,\n 'benedikt': 875,\n 'broad': 876,\n 'capable': 877,\n 'carl': 878,\n 'concern': 879,\n 'critical': 880,\n 'disruptive': 881,\n 'form': 882,\n 'frey': 883,\n 'generation': 884,\n 'imbued': 885,\n 'impact': 886,\n 'involve': 887,\n 'michael': 888,\n 'older': 889,\n 'osborne': 890,\n 'oxford': 891,\n 'predicted': 892,\n 'replacing': 893,\n 'risk': 894,\n 'sewing': 895,\n 'stirred': 896,\n 'swath': 897,\n 'timing': 898,\n 'writing': 899,\n 'alarm': 900,\n 'appeared': 901,\n 'arrive': 902,\n 'automated': 903,\n 'berkeley': 904,\n 'better': 905,\n 'cashier': 906,\n 'creation': 907,\n 'displace': 908,\n 'grew': 909,\n 'increasingly': 910,\n 'institute': 911,\n 'introduction': 912,\n 'paralegal': 913,\n 'prediction': 914,\n 'rank': 915,\n 'scanner': 916,\n 'sounded': 917,\n 'standard': 918,\n 'supermarket': 919,\n 'threaten': 920,\n 'today': 921,\n 'truck': 922,\n 'trucker': 923,\n 'vehicle': 924,\n 'comparable': 925,\n 'compared': 926,\n 'displacing': 927,\n 'employ': 928,\n 'employment': 929,\n 'ford': 930,\n 'full': 931,\n 'growing': 932,\n 'implied': 933,\n 'japanese': 934,\n 'least': 935,\n 'major': 936,\n 'martin': 937,\n 'period': 938,\n 'picture': 939,\n 'seeing': 940,\n 'seven': 941,\n 'strong': 942,\n 'top': 943,\n 'unemployment': 944,\n 'weak': 945,\n 'yet': 946,\n 'compete': 947,\n 'elsewhere': 948,\n 'era': 949,\n 'escaped': 950,\n 'expanding': 951,\n 'factory': 952,\n 'farm': 953,\n 'given': 954,\n 'indeed': 955,\n 'large': 956,\n 'namely': 957,\n 'nigeria': 958,\n 'obstacle': 959,\n 'poorer': 960,\n 'poverty': 961,\n 'productive': 962,\n 'represent': 963,\n 'robotic': 964,\n 'segment': 965,\n 'suffer': 966,\n 'sustain': 967,\n 'unless': 968,\n 'unusual': 969,\n 'australia': 970,\n 'baby': 971,\n 'begun': 972,\n 'bonus': 973,\n 'boost': 974,\n 'career': 975,\n 'cultural': 976,\n 'desire': 977,\n 'directly': 978,\n 'door': 979,\n 'incentive': 980,\n 'ineffective': 981,\n 'join': 982,\n 'kid': 983,\n 'led': 984,\n 'lift': 985,\n 'opening': 986,\n 'pursue': 987,\n 'raising': 988,\n 'return': 989,\n 'starting': 990,\n 'stronger': 991,\n 'success': 992,\n 'workforce': 993,\n 'admit': 994,\n 'admits': 995,\n 'annually': 996,\n 'backlash': 997,\n 'conservative': 998,\n 'controversy': 999,\n ...}"},"metadata":{}}]},{"source":"# Select the id for \"computer\": computer_id\ncomputer_id = dictionary.token2id.get(\"computer\") \nprint(computer_id)\n\n# Use computer_id with the dictionary to print the word\nprint(dictionary.get(computer_id))","metadata":{"executionTime":127,"lastSuccessfullyExecutedCode":"# Select the id for \"computer\": computer_id\ncomputer_id = dictionary.token2id.get(\"computer\") \nprint(computer_id)\n\n# Use computer_id with the dictionary to print the word\nprint(dictionary.get(computer_id))"},"cell_type":"code","id":"2a6be079-90fa-4bbe-8840-c6a061322dee","execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":"351\ncomputer\n"}]},{"source":"## Using the **_dictionary_** creating **_Gensim Corpus_**\n- Gensim uses a simple bag-of-words model which transforms each document into a bag of words using the token ids and the frequency of each token in the document.","metadata":{},"cell_type":"markdown","id":"99dc80ee-0ffb-4132-a0ab-a4ec40385ef5"},{"source":"# Create a MmCorpus: corpus \ncorpus = [dictionary.doc2bow(tok_doc) for tok_doc in tokenized_documents]\n\n# Print the first 5 word ids with their frequency counts from the fifth document\nprint(corpus[4][:5])","metadata":{"executionTime":105,"lastSuccessfullyExecutedCode":"# Create a MmCorpus: corpus \ncorpus = [dictionary.doc2bow(tok_doc) for tok_doc in tokenized_documents]\n\n# Print the first 5 word ids with their frequency counts from the fifth document\nprint(corpus[4][:5])"},"cell_type":"code","id":"210ed4d9-f9b5-42c2-b264-a83e532e0511","execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":"[(9, 1), (15, 1), (37, 1), (38, 1), (39, 1)]\n"}]},{"source":"Gensim corpus is a list of lists, each list item representing one document. Each document a series of tuples, the first item representing the tokenid from the dictionary and the second item representing the token frequency in the document.","metadata":{},"cell_type":"markdown","id":"48652911-4435-4126-8bfc-c95f17f48060"},{"source":"# Gensim bag-of-words","metadata":{},"cell_type":"markdown","id":"35650636-a1c2-47c3-9e49-10bd06ba576e"},{"source":"from collections import defaultdict\n\"\"\"defaultdict means that if a key is not found in the dictionary,\nthen instead of a KeyError being thrown, a new entry is created. \nThe type of this new entry is given by the argument of defaultdict\"\"\"","metadata":{"executionTime":145,"lastSuccessfullyExecutedCode":"from collections import defaultdict\n\"\"\"defaultdict means that if a key is not found in the dictionary,\nthen instead of a KeyError being thrown, a new entry is created. \nThe type of this new entry is given by the argument of defaultdict\"\"\""},"cell_type":"code","id":"e3f35025-3430-4aff-9385-5381b3f3e7a4","execution_count":46,"outputs":[{"output_type":"execute_result","execution_count":46,"data":{"text/plain":"'defaultdict means that if a key is not found in the dictionary,\\nthen instead of a KeyError being thrown, a new entry is created. \\nThe type of this new entry is given by the argument of defaultdict'"},"metadata":{}}]},{"source":"# Save the 66th document: doc\ndoc = corpus[65]\nprint(doc)\n\nprint()\n\n# Sort the doc for frequency: bow_doc\nbow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\nprint(bow_doc)","metadata":{"executionTime":110,"lastSuccessfullyExecutedCode":"# Save the 66th document: doc\ndoc = corpus[65]\nprint(doc)\n\nprint()\n\n# Sort the doc for frequency: bow_doc\nbow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\nprint(bow_doc)"},"cell_type":"code","id":"2c0ef629-2ae2-4d63-9eec-17a9b9c8fdd3","execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":"[(28, 1), (35, 1), (45, 1), (75, 1), (234, 1), (235, 1), (353, 1), (369, 1), (378, 2), (415, 1), (416, 1), (441, 1), (568, 2), (660, 1), (661, 1), (663, 1), (664, 1), (665, 1), (666, 1), (667, 1), (668, 1), (669, 1), (670, 2), (671, 1), (672, 1), (673, 1), (674, 1), (675, 1), (676, 1), (677, 1), (678, 1)]\n\n[(378, 2), (568, 2), (670, 2), (28, 1), (35, 1), (45, 1), (75, 1), (234, 1), (235, 1), (353, 1), (369, 1), (415, 1), (416, 1), (441, 1), (660, 1), (661, 1), (663, 1), (664, 1), (665, 1), (666, 1), (667, 1), (668, 1), (669, 1), (671, 1), (672, 1), (673, 1), (674, 1), (675, 1), (676, 1), (677, 1), (678, 1)]\n"}]},{"source":"**Note:**\n\nThe syntax of sorted: sorted(iterable, key=None, reverse=False)\n\n- key : A function that serves as a key for the sort comparison.","metadata":{},"cell_type":"markdown","id":"cdb40453-9e4d-4faa-812e-d655975ce189"},{"source":"# Print the top 5 words of the document alongside the count\nfor word_id, word_count in bow_doc[:5]:\n    print(dictionary.get(word_id), word_count)","metadata":{"executionTime":124,"lastSuccessfullyExecutedCode":"# Print the top 5 words of the document alongside the count\nfor word_id, word_count in bow_doc[:5]:\n    print(dictionary.get(word_id), word_count)"},"cell_type":"code","id":"1b3d0fb9-eef0-4140-ac4d-78c0431b41c4","execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":"traditional 2\nproject 2\npart 2\ncalled 1\nservice 1\n"}]},{"source":"# Import the itertools module\nimport itertools\n\n# Create the defaultdict: \"total_word_count\" in which the keys are all the token ids (word_id) and the values are the sum of their occurrence across all documents (word_count)\ntotal_word_count = defaultdict(int)\nfor word_id, word_count in itertools.chain.from_iterable(corpus):\n    total_word_count[word_id] += word_count","metadata":{"executionTime":117,"lastSuccessfullyExecutedCode":"# Import the itertools module\nimport itertools\n\n# Create the defaultdict: \"total_word_count\" in which the keys are all the token ids (word_id) and the values are the sum of their occurrence across all documents (word_count)\ntotal_word_count = defaultdict(int)\nfor word_id, word_count in itertools.chain.from_iterable(corpus):\n    total_word_count[word_id] += word_count"},"cell_type":"code","id":"a255a25b-94fd-46e9-b396-afe5839607ca","execution_count":52,"outputs":[]},{"source":"# Create a sorted list from the defaultdict: sorted_word_count\nsorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n\n# Print the top 5 words across all documents alongside the count\nfor word_id, word_count in sorted_word_count[:5]:\n    print(dictionary.get(word_id), word_count)","metadata":{"executionTime":164,"lastSuccessfullyExecutedCode":"# Create a sorted list from the defaultdict: sorted_word_count\nsorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n\n# Print the top 5 words across all documents alongside the count\nfor word_id, word_count in sorted_word_count[:5]:\n    print(dictionary.get(word_id), word_count)"},"cell_type":"code","id":"61dddf1d-4302-4037-9a0a-bfe9b123fb53","execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":"said 29\nrobot 28\npopulation 22\nnews 19\nhuman 16\n"}]},{"source":"# Tf-idf with gensim\n\nTF-IDF Formula: \n\n![image-3](image-3.png)\n\nThe weight of token i in document j is calculated by taking the term frequency (or how many times the token appears in the document) multiplied by the log of the total number of documents divided by the number of documents that contain the same term.","metadata":{},"cell_type":"markdown","id":"6acc5ecc-8c8e-4d39-b851-5d7e3cc81dd5"},{"source":"from gensim.models.tfidfmodel import TfidfModel\n\n# Passing corpus to TfidfModel : tfidf\ntfidf = TfidfModel(corpus)\ntfidf","metadata":{"executionTime":82,"lastSuccessfullyExecutedCode":"from gensim.models.tfidfmodel import TfidfModel\n\n# Passing corpus to TfidfModel : tfidf\ntfidf = TfidfModel(corpus)\ntfidf"},"cell_type":"code","id":"a137eb5f-b78f-4a0d-be2a-9c9661d8094b","execution_count":56,"outputs":[{"output_type":"execute_result","execution_count":56,"data":{"text/plain":"<gensim.models.tfidfmodel.TfidfModel at 0x7f536d206280>"},"metadata":{}}]},{"source":"# Reference each document by using it like a dictionary key with our new tfidf model\ntfidf[corpus[76]]","metadata":{"executionTime":131,"lastSuccessfullyExecutedCode":"# Reference each document by using it like a dictionary key with our new tfidf model\ntfidf[corpus[76]]"},"cell_type":"code","id":"3cd20037-2a39-438f-aaf4-19144c4bf765","execution_count":60,"outputs":[{"output_type":"execute_result","execution_count":60,"data":{"text/plain":"[(35, 0.08402491269879196),\n (88, 0.08402491269879196),\n (93, 0.11013281577999172),\n (122, 0.09439267703990184),\n (127, 0.07303463961632564),\n (152, 0.07628112737462436),\n (162, 0.07303463961632564),\n (163, 0.11013281577999172),\n (164, 0.2191039188489769),\n (171, 0.08877477835641555),\n (193, 0.1348334818656787),\n (202, 0.09439267703990184),\n (278, 0.05325710938059205),\n (305, 0.11013281577999172),\n (312, 0.10126842933820669),\n (350, 0.10126842933820669),\n (403, 0.09439267703990184),\n (405, 0.08877477835641555),\n (433, 0.1226264667617829),\n (597, 0.11013281577999172),\n (643, 0.11013281577999172),\n (663, 0.08877477835641555),\n (715, 0.11013281577999172),\n (724, 0.11013281577999172),\n (734, 0.09439267703990184),\n (766, 0.10126842933820669),\n (792, 0.1226264667617829),\n (797, 0.09439267703990184),\n (802, 0.08877477835641555),\n (844, 0.22026563155998344),\n (859, 0.2831780311197055),\n (874, 0.14398450418535907),\n (875, 0.14398450418535907),\n (876, 0.14398450418535907),\n (877, 0.14398450418535907),\n (878, 0.14398450418535907),\n (879, 0.1226264667617829),\n (880, 0.14398450418535907),\n (881, 0.14398450418535907),\n (882, 0.14398450418535907),\n (883, 0.14398450418535907),\n (884, 0.2452529335235658),\n (885, 0.14398450418535907),\n (886, 0.11013281577999172),\n (887, 0.14398450418535907),\n (888, 0.14398450418535907),\n (889, 0.14398450418535907),\n (890, 0.14398450418535907),\n (891, 0.1226264667617829),\n (892, 0.14398450418535907),\n (893, 0.14398450418535907),\n (894, 0.11013281577999172),\n (895, 0.14398450418535907),\n (896, 0.14398450418535907),\n (897, 0.14398450418535907),\n (898, 0.14398450418535907),\n (899, 0.14398450418535907)]"},"metadata":{}}]},{"source":"For the 77th document in our corpora, we see the token weights along with their token ids.","metadata":{},"cell_type":"markdown","id":"3f41d641-2b46-45c4-ab67-100aa87ed5c8"},{"source":"# Sort the weights from highest to lowest: sorted_tfidf_weights\nsorted_tfidf_weights = sorted(tfidf[corpus[76]], key=lambda w: w[1], reverse=True)\n\n# Print the top 5 weighted words\nfor term_id, weight in sorted_tfidf_weights[:5]:\n    print(dictionary.get(term_id), weight)","metadata":{"executionTime":150,"lastSuccessfullyExecutedCode":"# Sort the weights from highest to lowest: sorted_tfidf_weights\nsorted_tfidf_weights = sorted(tfidf[corpus[76]], key=lambda w: w[1], reverse=True)\n\n# Print the top 5 weighted words\nfor term_id, weight in sorted_tfidf_weights[:5]:\n    print(dictionary.get(term_id), weight)"},"cell_type":"code","id":"01e9d164-ae12-44fc-b350-eef4a8152b82","execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":"job 0.2831780311197055\ngeneration 0.2452529335235658\nautomation 0.22026563155998344\nmachine 0.2191039188489769\nauthority 0.14398450418535907\n"}]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}