{"cells":[{"source":"# Word counts with bag-of-words\n- Basic method for finding topics in a text\n- Need to first create tokens using tokenization\n- ... and then count up all the tokens\n- **The more frequent a word, the more important it might be**","metadata":{},"cell_type":"markdown","id":"4d2fc11d-14a8-4161-bc30-d97f0b82432f"},{"source":"import zipfile\n\nzip_path = \"datasets/News%20articles.zip\"  \ndestination_path = \"datasets/news_article\" \n\n# Open the zip file\nwith zipfile.ZipFile(zip_path, 'r') as zip_ref:\n    # Extract all contents of the zip file to the destination folder\n    zip_ref.extractall(destination_path)\n","metadata":{"executionTime":70,"lastSuccessfullyExecutedCode":"import zipfile\n\nzip_path = \"datasets/News%20articles.zip\"  \ndestination_path = \"datasets/news_article\" \n\n# Open the zip file\nwith zipfile.ZipFile(zip_path, 'r') as zip_ref:\n    # Extract all contents of the zip file to the destination folder\n    zip_ref.extractall(destination_path)\n"},"cell_type":"code","id":"1986613b-be01-4eef-b9aa-7a87197c82a6","execution_count":2,"outputs":[]},{"source":"file_path = \"datasets/news_article/News articles/articles.txt\"  \n\n# Open the text file in read mode\nwith open(file_path, 'r') as file:\n    # Read the contents of the file\n    article = file.read()","metadata":{"executionTime":69,"lastSuccessfullyExecutedCode":"file_path = \"datasets/news_article/News articles/articles.txt\"  \n\n# Open the text file in read mode\nwith open(file_path, 'r') as file:\n    # Read the contents of the file\n    article = file.read()"},"cell_type":"code","id":"3ef67ae0-908a-4d53-8407-a753636ba346","execution_count":33,"outputs":[]},{"source":"import nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\n# Import Counter\nfrom collections import Counter\n\n# Tokenize the article: tokens\ntokens = word_tokenize(article)\n\n# Convert the tokens into lowercase: lower_tokens\nlower_tokens = [t.lower() for t in tokens]\n\n# Create a Counter with the lowercase tokens: bow_simple\nbow_simple = Counter(lower_tokens)\n\n# Print the 10 most common tokens\nprint(bow_simple.most_common(10))","metadata":{"executionTime":109,"lastSuccessfullyExecutedCode":"import nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\n# Import Counter\nfrom collections import Counter\n\n# Tokenize the article: tokens\ntokens = word_tokenize(article)\n\n# Convert the tokens into lowercase: lower_tokens\nlower_tokens = [t.lower() for t in tokens]\n\n# Create a Counter with the lowercase tokens: bow_simple\nbow_simple = Counter(lower_tokens)\n\n# Print the 10 most common tokens\nprint(bow_simple.most_common(10))"},"cell_type":"code","id":"90cbd023-3ce8-4b6b-b4e1-69c568c59124","execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":"[('the', 274), (',', 269), ('.', 189), ('to', 131), ('of', 119), ('a', 100), ('in', 99), ('and', 80), ('that', 67), ('â€™', 54)]\n"},{"output_type":"stream","name":"stderr","text":"[nltk_data] Downloading package punkt to /home/repl/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"}]},{"source":"# Simple text preprocessing\n- Tokenization to create bag-of-words\n- Lowercasing words\n- Lemmatization/Stemming -> shorten words to their root stems\n- Removing stop words, punctuations, unwanted tokens","metadata":{},"cell_type":"markdown","id":"dbe12b42-a66c-4857-9bd0-82a804e267a4"},{"source":"- Remove stop words and non-alphabetic characters, lemmatize, and perform a new bag-of-words on your cleaned text.\n\nYou start with the same tokens you created in the last exercise: `lower_tokens`. ","metadata":{},"cell_type":"markdown","id":"ec43ef4b-c2d7-4fa0-b2d3-fe962df1b3cd"},{"source":"file_path = 'datasets/english_stopwords.txt'\n\n# Open the text file in read mode\nwith open(file_path, 'r') as file:\n    # Read the contents of the file\n    english_stop = file.read()","metadata":{"executionTime":27,"lastSuccessfullyExecutedCode":"file_path = 'datasets/english_stopwords.txt'\n\n# Open the text file in read mode\nwith open(file_path, 'r') as file:\n    # Read the contents of the file\n    english_stop = file.read()"},"cell_type":"code","id":"5611f6d9-854d-4f51-b222-9317162f42b2","execution_count":4,"outputs":[]},{"source":"# Creating a list of stop_words\nenglish_stop = english_stop.split('\\n')","metadata":{"executionTime":50,"lastSuccessfullyExecutedCode":"# Creating a list of stop_words\nenglish_stop = english_stop.split('\\n')"},"cell_type":"code","id":"acb84527-78e5-4888-9335-a13ba0ce0805","execution_count":7,"outputs":[]},{"source":"# Import WordNetLemmatizer and download WordNet resource\nimport nltk\nnltk.download('omw-1.4')\nnltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\n\n# Retain alphabetic words: alpha_only\nalpha_only = [t for t in lower_tokens if t.isalpha()]\n\n# Remove all stop words: no_stops\nno_stops = [t for t in alpha_only if t not in english_stop]\n\n# Instantiate the WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\n\n# Lemmatize all tokens into a new list: lemmatized\nlemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n\n# Create the bag-of-words: bow\nbow = Counter(lemmatized)\n\n# Print the 10 most common tokens\nprint(bow.most_common(10))","metadata":{"executionTime":1527,"lastSuccessfullyExecutedCode":"# Import WordNetLemmatizer and download WordNet resource\nimport nltk\nnltk.download('omw-1.4')\nnltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\n\n# Retain alphabetic words: alpha_only\nalpha_only = [t for t in lower_tokens if t.isalpha()]\n\n# Remove all stop words: no_stops\nno_stops = [t for t in alpha_only if t not in english_stop]\n\n# Instantiate the WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\n\n# Lemmatize all tokens into a new list: lemmatized\nlemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n\n# Create the bag-of-words: bow\nbow = Counter(lemmatized)\n\n# Print the 10 most common tokens\nprint(bow.most_common(10))"},"cell_type":"code","id":"ae2af0cb-736b-478b-b4f3-dd6bcdd1cd93","execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":"[nltk_data] Downloading package omw-1.4 to /home/repl/nltk_data...\n[nltk_data] Downloading package wordnet to /home/repl/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"},{"output_type":"stream","name":"stdout","text":"[('said', 29), ('robot', 28), ('population', 22), ('news', 19), ('human', 16), ('growth', 16), ('fake', 15), ('country', 14), ('united', 14), ('machine', 13)]\n"}]},{"source":"# Introduction to gensim\n- Popular open source NLP library\n- Uses top academic model to perform complex tasks:\n    - Building document or word vectors\n    - Performing topic identification or document comparison","metadata":{},"cell_type":"markdown","id":"84a78cbf-da1a-44b6-99ed-d53809cf7f8d"},{"source":"## Word Vectors\n\n![image-2](image-2.png)\n\nWith these vectors, we can then see relationships among the words or documents based on how near or far they are and also what similar comparisons we find. For example, in this graphic we can see that the vector operation king minus queen is approximately equal to man minus woman. Or that Spain is to Madrid as Italy is to Rome. ","metadata":{},"cell_type":"markdown","id":"b9671387-eba7-45bb-ae8e-faa09008845e"},{"source":"# Creating the list of documents\narticle = article.split(\"\\n\\n\\n\")","metadata":{"executionTime":64,"lastSuccessfullyExecutedCode":"# Creating the list of documents\narticle = article.split(\"\\n\\n\\n\")"},"cell_type":"code","id":"b084ad9b-1160-490d-b749-17b7ea83a826","execution_count":35,"outputs":[]},{"source":"import nltk\nfrom nltk.tokenize import word_tokenize\n\ntokenized_documents = []\n\n# Tokenize each document in the list\nfor doc in article:\n    tokens = word_tokenize(doc)\n    tokenized_documents.append(tokens)","metadata":{"executionTime":98,"lastSuccessfullyExecutedCode":"import nltk\nfrom nltk.tokenize import word_tokenize\n\ntokenized_documents = []\n\n# Tokenize each document in the list\nfor doc in article:\n    tokens = word_tokenize(doc)\n    tokenized_documents.append(tokens)"},"cell_type":"code","id":"b0ea6534-21b4-4ead-9d1d-36c3ea2a5644","execution_count":38,"outputs":[]},{"source":"# Import Dictionary\nfrom gensim.corpora.dictionary import Dictionary\n\n# Create a Dictionary from the articles: dictionary\ndictionary = Dictionary(tokenized_documents)\ndictionary","metadata":{"executionTime":105,"lastSuccessfullyExecutedCode":"# Import Dictionary\nfrom gensim.corpora.dictionary import Dictionary\n\n# Create a Dictionary from the articles: dictionary\ndictionary = Dictionary(tokenized_documents)\ndictionary"},"cell_type":"code","id":"c3764a02-19a0-4008-bdf8-f241f135b727","execution_count":39,"outputs":[{"output_type":"execute_result","execution_count":39,"data":{"text/plain":"<gensim.corpora.dictionary.Dictionary at 0x7f01d8773f40>"},"metadata":{}}]},{"source":"# Select the id for \"computer\": computer_id\ncomputer_id = dictionary.token2id.get(\"computer\")\nprint(computer_id)\n\n# Use computer_id with the dictionary to print the word\nprint(dictionary.get(computer_id))","metadata":{"executionTime":86,"lastSuccessfullyExecutedCode":"# Select the id for \"computer\": computer_id\ncomputer_id = dictionary.token2id.get(\"computer\")\nprint(computer_id)\n\n# Use computer_id with the dictionary to print the word\nprint(dictionary.get(computer_id))"},"cell_type":"code","id":"2a6be079-90fa-4bbe-8840-c6a061322dee","execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":"825\ncomputer\n"}]},{"source":"# Create a MmCorpus: corpus \n# i.e for each tokenized document from the list, its creating bag of words (bow) --> for each tokenized document from the list, its creating the word with its frequencies\ncorpus = [dictionary.doc2bow(tok_doc) for tok_doc in tokenized_documents]\n\n# Print the first 5 word ids with their frequency counts from the fifth document\nprint(corpus[4][:5])","metadata":{"executionTime":86,"lastSuccessfullyExecutedCode":"# Create a MmCorpus: corpus \n# i.e for each tokenized document from the list, its creating bag of words (bow) --> for each tokenized document from the list, its creating the word with its frequencies\ncorpus = [dictionary.doc2bow(tok_doc) for tok_doc in tokenized_documents]\n\n# Print the first 5 word ids with their frequency counts from the fifth document\nprint(corpus[4][:5])"},"cell_type":"code","id":"210ed4d9-f9b5-42c2-b264-a83e532e0511","execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":"[(2, 1), (14, 2), (18, 1), (27, 1), (50, 1)]\n"}]},{"source":"# Gensim bag-of-words","metadata":{},"cell_type":"markdown","id":"35650636-a1c2-47c3-9e49-10bd06ba576e"},{"source":"from collections import defaultdict\n\"\"\"defaultdict means that if a key is not found in the dictionary,\nthen instead of a KeyError being thrown, a new entry is created. \nThe type of this new entry is given by the argument of defaultdict\"\"\"","metadata":{"executionTime":296,"lastSuccessfullyExecutedCode":"from collections import defaultdict\n\"\"\"defaultdict means that if a key is not found in the dictionary,\nthen instead of a KeyError being thrown, a new entry is created. \nThe type of this new entry is given by the argument of defaultdict\"\"\""},"cell_type":"code","id":"e3f35025-3430-4aff-9385-5381b3f3e7a4","execution_count":46,"outputs":[{"output_type":"execute_result","execution_count":46,"data":{"text/plain":"'defaultdict means that if a key is not found in the dictionary,\\nthen instead of a KeyError being thrown, a new entry is created. \\nThe type of this new entry is given by the argument of defaultdict'"},"metadata":{}}]},{"source":"","metadata":{},"cell_type":"code","id":"2c0ef629-2ae2-4d63-9eec-17a9b9c8fdd3","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}