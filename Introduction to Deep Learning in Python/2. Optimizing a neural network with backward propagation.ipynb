{"cells":[{"source":"# The need for optimization","metadata":{},"cell_type":"markdown","id":"d96eda47-9b35-4462-9e27-b79d7dba39f6"},{"source":"## Loss Function\n![image-3](image-3.png)\n\n- Lower loss function value means a better model\n- Goal: Find the weights that give the lowest value for the loss function\n- **Gradient descent**","metadata":{},"cell_type":"markdown","id":"ff3aa183-df9a-4daf-8280-ee6ff91b0c5d"},{"source":"## Gradient descent\n- Start at random point\n- Until you are somewhere flat:\n    - Find the slope\n    - Take a step downhill","metadata":{},"cell_type":"markdown","id":"7d04d912-b252-4cfd-905c-6cb2cd8e2d80"},{"source":"## Coding how weight changes affect accuracy\n![image-4](image-4.png)\n","metadata":{},"cell_type":"markdown","id":"05ef51c7-1a11-421d-868a-50f6d416d579"},{"source":"import numpy as np\n\n# Input nodes\ninputs = np.array([0,3])\n\n# Initial weights\nweights = {'weight_0': np.array([2,1]),\n           'weight_1': np.array([1,2]),\n           'output': np.array([1,1])}\n\n# Target value\ntarget = np.array(3)","metadata":{"executionTime":72,"lastSuccessfullyExecutedCode":"import numpy as np\n\n# Input nodes\ninputs = np.array([0,3])\n\n# Initial weights\nweights = {'weight_0': np.array([2,1]),\n           'weight_1': np.array([1,2]),\n           'output': np.array([1,1])}\n\n# Target value\ntarget = np.array(3)"},"cell_type":"code","id":"3a4559e3-f560-43d1-9673-1b60262218a6","execution_count":1,"outputs":[]},{"source":"# Function that calculates value at each node\ndef calc_nodes(inputs, weights):\n    node_0 = (inputs*weights['weight_0']).sum()\n    node_1 = (inputs*weights['weight_1']).sum()\n    hidden = np.array([node_0,node_1])\n    output = (hidden*weights['output']).sum()\n    return output","metadata":{"executionTime":92,"lastSuccessfullyExecutedCode":"# Function that calculates value at each node\ndef calc_nodes(inputs, weights):\n    node_0 = (inputs*weights['weight_0']).sum()\n    node_1 = (inputs*weights['weight_1']).sum()\n    hidden = np.array([node_0,node_1])\n    output = (hidden*weights['output']).sum()\n    return output"},"cell_type":"code","id":"3d5b00f0-540a-413c-a356-cb881ee29166","execution_count":2,"outputs":[]},{"source":"# Calculate output at initial weight\ninit_weight = calc_nodes(inputs, weights)\n\n# Error for initial weight\nerror_0 = init_weight - target\nerror_0","metadata":{"executionTime":57,"lastSuccessfullyExecutedCode":"# Calculate output at initial weight\ninit_weight = calc_nodes(inputs, weights)\n\n# Error for initial weight\nerror_0 = init_weight - target\nerror_0"},"cell_type":"code","id":"9d94f967-9c9a-45e8-b2ba-7959be3038fe","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"6"},"metadata":{}}]},{"source":"# Update the weights\nupt_weights = {'weight_0': np.array([2,1]),\n           'weight_1': np.array([1,1]),\n           'output': np.array([0.5,0.5])}\n\n# Calculate for updated weights\npred = calc_nodes(inputs, upt_weights)\n\n# Error for updated weight\nerror_1 = pred - target\nerror_1","metadata":{"executionTime":81,"lastSuccessfullyExecutedCode":"# Update the weights\nupt_weights = {'weight_0': np.array([2,1]),\n           'weight_1': np.array([1,1]),\n           'output': np.array([0.5,0.5])}\n\n# Calculate for updated weights\npred = calc_nodes(inputs, upt_weights)\n\n# Error for updated weight\nerror_1 = pred - target\nerror_1"},"cell_type":"code","id":"c405888a-bb4a-4847-9f8f-20db0a66718b","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"0.0"},"metadata":{}}]},{"source":"# Gradient Descent\n\n![gradient_descent_LR_0.2.gif](gradient_descent_LR_0.2.gif)\n\n- With gradient descent, you repeatedly found a slope capturing how your loss function changes as a weight changes. Then, make a small change to the weight to get to a lower point and repeat this until you cannot go downhill any more. \n\n![image-6](image-6.png)\n\n- If the slope is positive, going opposite the slope means moving to lower numbers.\n- Subtracting the slope from the current value achieves this. But too big a step might lead us far astray.\n- So, instead of directly subtracting the slope, we multiply the slope by a small number, called the learning rate, and we change the weight by the product of that multiplication. ","metadata":{},"cell_type":"markdown","id":"7019b515-abfd-45e3-ad0a-aa69ea26ddbc"},{"source":"## Slope calculation example\n![image-8](image-8.png)\n\n- To calculate the slope for a weight, need to multiply:\n1. **Slope of the loss function w.r.t value at the node we feed into**\n- eg. For mean_squared loss function : Slope of mean-squared loss function w.r.t prediction\n- i.e  2 (Predicted Value - Actual Value) = 2 x Error ; here: 2 * -4\n2. **The value of the node that feed into our weight** ; here: 3\n3. **Slope of the activation function w.r.t value we feed into** ; here: none\nNote: for the ReLU function, the slope is 0 if the input into a node is negative. If the input into the node is positive, the output is the same as the input. So the slope would be 1.\n\nThus, slope = 2*-4*3 = -24\n- If `learning rate` is `0.01`, the new weight would be: 2 - 0.01(-24) = 2.24","metadata":{},"cell_type":"markdown","id":"cfcc3f83-8be0-49c0-8922-f911655c3b59"},{"source":"from numpy import array\n\n# Initializing values\ninput_data = array([1, 2, 3])\nweights = array([0, 2, 1])\ntarget = 0 ","metadata":{"executionTime":0,"lastSuccessfullyExecutedCode":"from numpy import array\n\n# Initializing values\ninput_data = array([1, 2, 3])\nweights = array([0, 2, 1])\ntarget = 0 "},"cell_type":"code","id":"e209c6da-6660-481c-af09-493b97c745a3","execution_count":5,"outputs":[]},{"source":"# Calculate the predictions: preds\npreds = (input_data * weights).sum()\n\n# Calculate the error: error\nerror = preds - target\n\n# Calculate the slope: slope\nslope = 2 * error * input_data\n\n# Print the slope\nprint(slope)","metadata":{"executionTime":60,"lastSuccessfullyExecutedCode":"# Calculate the predictions: preds\npreds = (input_data * weights).sum()\n\n# Calculate the error: error\nerror = preds - target\n\n# Calculate the slope: slope\nslope = 2 * error * input_data\n\n# Print the slope\nprint(slope)"},"cell_type":"code","id":"2176cc9e-3bc8-4190-988e-789dd34bae71","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":"[14 28 42]\n"}]},{"source":"# Set the learning rate: learning_rate\nlearning_rate = 0.01\n\n# Calculate the predictions: preds\npreds = (weights * input_data).sum()\n\n# Calculate the error: error\nerror = preds - target\n\n# Calculate the slope: slope\nslope = 2 * error * input_data\n\n# Update the weights: weights_updated\nweights_updated = weights - (slope * learning_rate) #<-- gradient descent\n\n# Get updated predictions: preds_updated\npreds_updated = (input_data*weights_updated).sum()\n\n# Calculate updated error: error_updated\nerror_updated = preds_updated-target\n\n# Print the original error\nprint(error)\n\n# Print the updated error\nprint(error_updated)","metadata":{"executionTime":62,"lastSuccessfullyExecutedCode":"# Set the learning rate: learning_rate\nlearning_rate = 0.01\n\n# Calculate the predictions: preds\npreds = (weights * input_data).sum()\n\n# Calculate the error: error\nerror = preds - target\n\n# Calculate the slope: slope\nslope = 2 * error * input_data\n\n# Update the weights: weights_updated\nweights_updated = weights - (slope * learning_rate) #<-- gradient descent\n\n# Get updated predictions: preds_updated\npreds_updated = (input_data*weights_updated).sum()\n\n# Calculate updated error: error_updated\nerror_updated = preds_updated-target\n\n# Print the original error\nprint(error)\n\n# Print the updated error\nprint(error_updated)"},"cell_type":"code","id":"a59cff3a-5e0c-4a95-96bb-4c5d3e5192f5","execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":"7\n5.04\n"}]},{"source":"# Backpropagation\n![image-7](image-7.png)\n\n- It calculates the necessary slopes sequentially from the weights closest to the prediction, through the hidden layers, eventually back to the weights coming from the inputs\n- Allows gradient descent to update all weights in neural network (by getting gradients for all weights)\n- Comes from chain rule of calculus","metadata":{},"cell_type":"markdown","id":"ffd69b08-66c8-4687-9404-a60c37926990"},{"source":"## Example of backpropagation\n1. Start at some random set of weights\n2. Use forward propagation to make a prediction\n\n![image-9](image-9.png)\n\n3. Use backward propagation to calculate the slope of the loss function w.r.t each weight\n- Calculating the slopes: \n    - For node '1': 2 x error x input x slope of activation function = 2 x 3 x 1 = 6 i.e gradient for weight 1 is 6\n    - For node '3': 2 x error x input x slope of activation function = 2 x 3 x 3 = 18 i.e gradient for weight 2 is 18\n4. Multiply that slope by the learning rate, and subtract from the current weights\n5. Keep going with that cycle until we get to a flat part\n","metadata":{},"cell_type":"markdown","id":"3e1c528f-e904-454f-a835-1bba2ecf4cfa"}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}